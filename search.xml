<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[Python的小伙伴们之numpy]]></title>
    <url>%2F2019%2F01%2F26%2FPython%E4%B9%8Bnumpy%2F</url>
    <content type="text"><![CDATA[numpy.random 参考于: https://docs.scipy.org/doc/numpy-1.15.1/reference/routines.random.html https://www.jianshu.com/p/214798dd8f93numpy.random.rand() numpy.random.rand(d0,d1,…,dn) 根据给定维度生成[0,1)之间的数据，包含0，不包含1 返回值为指定维度的array 1234np.random.rand(3,2)# array([[0.77342926, 0.13062061],# [0.04889282, 0.66763161],# [0.72599757, 0.27826069]]) numpy.random.randn() numpy.random.randn(d0,d1,…,dn) 返回一个或一组样本，具有标准正态分布。 返回值为指定维度的array 1234567np.random.randn() # 没有参数时,返回单个数# -1.3281108519652927np.random.randn(3,2)# array([[ 2.11802305, 2.85387116],# [-0.01947161, -2.09343863],# [-1.96342724, 0.01979386]]) numpy.random.randint() numpy.random.randint(low, high=None, size=None, dtype=’l’) 返回随机整数，范围区间为[low,high），包含low，不包含high dtype为数据类型,默认是np.int 没有high时,默认的是[0,low) 123456np.random.randint(2, size = 10)# array([1, 1, 1, 1, 0, 1, 1, 0, 1, 0])np.random.randint(1, 5, size = (2,4))# array([[3, 1, 2, 2],# [3, 2, 4, 1]]) numpy.random.random_sample()numpy.random.random()numpy.random.ranf()numpy.random.sample() numpy.random.—(size = None) 返回[0.0, 1.0]区间的浮点数 1234567np.random.random_sample()# 0.43535270552664374np.random.ranf((3,2))# array([[0.07730948, 0.19447818],# [0.66644615, 0.41688169],# [0.92430307, 0.78935612]]) numpy.random.choice() numpy.random.choice(a, size = None, replace = True, p = None) a:一位数组或一个int值,一维数组的话就是从数组中随机生成元素,int的话就类似于np.arange(a) size:int值或者int元组,为输出形状 replace:是否可以出现重复值,默认为True,False的话就不允许有重复 p:a中的每个数出现的概率,默认为均匀分布的 返回值为指定维度的array 12345678910111213# 生成大小为3,范围为np.arange(5)np.random.choice(5,3)# array([4, 3, 3])# 生成大小为3,范围为np.arange(5)的非均匀的np.random.choice(5,3,p = [0.1, 0, 0.6, 0.2, 0.1])# array([3, 0, 2])# 由数组生成aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])# array(['pooh', 'Christopher', 'piglet', 'Christopher', 'pooh'],# dtype='&lt;U11') numpy.random.seed numpy.random.seed(seed = None) seed:指定seed值,将会得到相同的随机结果 1234567np.random.seed(1000)np.random.randint(5)# 3np.random.seed(1000)np.random.randint(5)# 3 其他 astype操作总是会返回一个新的数组 两个数组之间的运算是对应位置元素的运算;数组与标量的运算是涉及到每一个数组元素的;数组与数组之间的比较是产生一个bool数组 python的list与numpy的array的重要区别:list的切片是产生了一个新的list,而array的切片是产生一view,即对list的切片进行值的修改,不影响原list,但是nrray会影响 1234567891011list1 = [1,2,3,4,5,6]arr1 = np.array(list1)list2 = list1[:3]arr2 = arr1[:3]arr2[0] = 1234list2[0] = 1234# list1:[1,2,3,4,5,6]# list2:[1234, 2, 3]# arr1:array([1234, 2, 3, 4, 5, 6])# arr2:array([1234, 2, 3]) numpy.where() numpy.where(condition[, x, y]) condition:条件,数组的形式,满足时返回x的对应值,不满足时返回y x,y:数组形式 123456x = np.arange(9.).reshape(3, 3)np.where(x&lt;5, x, 0)# array([[0., 1., 2.],# [3., 4., 0.],# [0., 0., 0.]]) numpy.any检测数组中只要有一个ture返回就是true，而numpy.all检测数组中都是true才会返回true 123bools = np.array([False, False, True, False])np.any(bools) # Truenp.all(bools) # False numpy.unique 能返回排好序且不重复的值,相当于Python的sort(set(x)) 123ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])np.unique(ints)# array([1, 2, 3, 4]) numpy.in1d 测试一个数组的值是否在另一个数组里，返回一个布尔数组 123values = np.array([6, 0, 0, 3, 2, 5, 6])np.in1d(values, [2, 3, 6])# array([ True, False, False, True, True, False, True], dtype=bool) numpy.cumsum() numpy.cumsum(a, axis=None, dtype=None, out=None) 返回给定轴上的累加和 a:数组形式 axis:给定轴,默认是一维地去累加 返回一个ndarray 12345678910a = np.array([[1,2,3], [4,5,6]])# array([[1, 2, 3],# [4, 5, 6]])np.cumsum(a)# array([ 1, 3, 6, 10, 15, 21])np.cumsum(a,axis=0)# array([[1, 2, 3],# [5, 7, 9]])]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能复习笔记]]></title>
    <url>%2F2019%2F01%2F26%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[四大流派：符号主义(专家系统)，连接主义(人工神经网络)，行为主义(机器人)，统计主义(机器学习)机器学习：监督，半监督，非监督，强化，迁移线性回归 model： loss function： 参数更新： 逻辑回归 model： loss function： 参数更新： 推导为什么不能用均方差代替交叉熵： Cascading logistic regression model：为了解决逻辑回归不能处理异或问题，将多个逻辑回归连接起来，一部分负责特征转换，一部分负责分类，这样就就解决了异或问题；这样的每一个逻辑回归就是神经元，串起来形成了神经网络 如何加快模型训练 1特征缩放，特征归一化 2改变梯度下降方法：随机梯度下降，批量梯度下降 3微调学习率 什么是机器学习对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升 优化算法损失函数 平方损失函数更适合输出为连续，并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失则更适合二分类或多分类的场景。 均方差： 交叉熵： 凸函数凸函数曲面上任意两点连接而成的线段，其上的任意一点都不会处于该函数曲面的下方 梯度下降 推导： 经典的梯度下降法在每次对模型参数进行更新时，需要遍历所有的训练数据。当M很大时，这需要很大的计算量，耗费很长的计算时间，在实际应用中基本不可行。 随机梯度下降法用单个训练数据即可对模型参数进行一次更新，大大加快了收敛速率。 随机梯度下降法对梯度的估计常常出现偏差，造成目标函数曲线收敛得很不稳定，伴有剧烈波动，有时甚至出现不收敛的情况 为了降低随机梯度的方差，从而使得迭代算法更加稳定，也为了充分利用高度优化的矩阵运算操作，在实际应用中我们会同时处理若干训练数据，该方法被称为小批量梯度下降法。 小批量梯度下降的m通常选取2的幂次时能充分利用矩阵运算操作，先对数据进行随机排序，再顺序挑选m个训练数据，学习率通常先较大，后减小。 AdaGrad方法采用历史梯度平方和，其公式为： RMSProp公式： Adam方法将惯性保持和环境感知这两个优点集于一身。一方面记录了过往梯度与当前梯度的平均，体现了惯性保持，一方面记录了过往梯度平方与当前梯度平方的平均，体现了环境感知。 正则化L1:L2: Dropout Dropout相当于每次迭代都在训练不同结构的神经网络，可以被认为是一种实用的大规模深度神经网络的模型集成算法，减少过拟合风险，增强泛化能力 testing时没有dropout，training时dropout是p%的话，测试时所有的weights要乘以1-p% 过拟合和欠拟合 欠拟合：高bais error，低variance error；过拟合：低bais error，高variance error Bias error ≈ 训练集上的错误率，训练集上的错误率 = avoidable error + unavoidable error，Variance error ≈ 开发集（或测试集）上的表现比训练集上差多少 ， 过拟合就是在训练集上的表现很好，但在测试集和新数据上的表现较差；欠拟合指的是模型在训练和预测时表现都不好的情况。 降低过拟合方法：1获取更多数据2降低模型复杂度3正则化4集成学习方法5提前停止6dropout 降低欠拟合方法：1添加新特征2增加模型复杂度3减小正则化系数4新的激活函数5自适应的学习率6选择合适的损失函数 神经网络 反向传播用以更新模型参数;正向传播用以计算loss 激活函数 sigmoid及其导数： tanh及其导数： softmax及其导数： sigmoid梯度消失的原因 根据其导函数可知，当z很大或很小时，f(z)趋近于0，会造成梯度消失。 Relu相较于sigmoid的优点及局限性 Relu是Maxout的一个特殊情况 优点：1.sigmoid和tanh都需要进行指数的计算，复杂度高，而relu只需要一个阈值就可激活；2.relu的非饱和性可以有效地解决梯度消失的问题；3.relu的单侧抑制提供了网络的稀疏表达能力 局限性：在训练过程中导致神经元死亡的问题，该神经元的梯度永远为0，不再被激活，解决方法是relu的变种形式 神经网络的训练技巧 参数不能全部初始化为0，因为神经元结构相同，输入输出相同，无法训练，需要随机化参数来打破对称性； 如果不进行归一化处理，会致使网络在每一次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。归一化的方法是：针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），公式为： CNN 计算公式： 为什么选择CNN1.一些特征比整张图片要小很多，不需要看整张图–卷积 2.一些特征会在一张图的不同区域出现–卷积 3.二次抽样不会改变对象–池化 稀疏交互和参数共享稀疏交互的物理意义是先学习局部特征，再将局部特征组合为更复杂更抽象的特征。参数共享的物理意义是卷积层具有平移等变性。 池化包括均值池化和最大值池化，本质是降采样，而且能够保持对平移、伸缩、旋转操作的不变性。 为什么要deep deep-&gt;模块化-&gt;更少的训练数据 类似于决策树，每一层的每一个神经元相当于一个基本的分类器，后面的层的神经元使用前一层的来建立一个新的分类器。复杂的问题模块化 RNN序列数据举例语音识别，情感分析，DNA序列分析，机器翻译等 LSTM 输入门，输出门，遗忘门，输入门控制当前计算的新状态有多大程度更新到记忆单元中，遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉，输出门控制当前的输出有多大程度取决于当前的记忆单元。 遗忘门接近于1时，实现记忆功能，遗忘门接近于0时，实现的是遗忘功能。 三个门的激活函数为sigmoid，sigmoid函数输出在0~1之间，符合门控的物理定义。在生成候选记忆时，使用tanh，输出在-1~1之间，大多数场景下分布为0为中心相吻合，且在0附近有更大的梯度，收敛更快。 4个输入，1个输出 long short-term memory 可以理解为：比较长的-短时-记忆 LSTM可以解决RNN的梯度消失，但不能解决梯度爆炸 RNN对比CNN等前馈神经网络 一般的前馈神经网络，如CNN，可以捕捉到局部特征，但是序列中的依赖关系很难被学到，而RNN第t层的隐含状态h编码了序列中前t个输入的信息，最后一层的状态h编码了整个序列的信息，RNN具备对序列顺序信息的刻画能力，往往能得到更准确的结果。 RNN梯度消失、梯度爆炸问题 由于预测的误差是沿着神经网络的每一层反向传播的，因此当雅克比矩阵的最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸；反之，若雅克比矩阵的最大特征值小于1，梯度的大小会呈指数缩小，产生梯度消失。 梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值 时，对梯度进行等比收缩。 LSTM及GRU通过加入门控机制，很大程度上弥补了梯度消失带来的损失 迁移学习概念 把已学训练好的模型参数迁移到新的模型来帮助新模型训练，通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。 应用场景 同样的任务，不同的分布，或者不同的任务，同样的分布 源数据：有标签；目标数据：有标签(很少)；比如：源数据为许多人的语音，目标数据为特定用户的语音；fine-tuning，multitask lear 源数据：有标签；目标数据：无标签；比如：源数据为mnist，目标数据为一些不带标签的数字图像；domain-adversarial training,zero-shot learining 源数据：无标签；目标数据：有标签；self-taught learning 源数据：无标签；目标数据：无标签；self-taught clustering]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习笔记]]></title>
    <url>%2F2019%2F01%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[SVM线性SVM loss: hinge loss 凸函数 $l(f(x^n), y^n) = max(0, 1-y^nf(x^n))$, 更为常见的写法是用 $y^nf(x) \geq 1-e^n$, 这里的 $e^n$ 就是松弛变量 边界最宽的 Large-Margin 离边界最近的点就是支撑向量 二次规划(凸优化) 遗传算法 种群-&gt;幸运轮-&gt;存活者-&gt;配对重组变异-&gt;种群-&gt;… 遗传算子:配对 重组 变异 三个算子 算法实现 初始种群产生 个体存活规则:适应函数计算个体的适用度,然后根据概率选出存活的个体 配对(选择)算子 个体任意配对,未考虑适应度高的个体更具有吸引力 按适应度排序,然后相邻的两两配对 概率选择,保留了高质量个体与低质量个体配对的可能,增加了个体的多样性,但是最终优秀个体可能会消失,解决办法是一部分最优秀的个体直接复制到下一代,不经过重组变异 重组(交叉) 单点交叉 两点交叉 均匀交叉 实际80%重组,20%直接复制到下一代 变异:随机选择一位求反,只有配对和交叉,就不存在新个体,会陷入局部最优解的 KNN选择 生成种群:对d维特征进行二进制编码,包含该特征即设为1 适应函数: $f=1/(C_1E_R + C_2N_A)$ 其中$E_R$是分类错误的样本数. $N_A$ 是保留属性的数目. $C_1C_2$ 是可调的参数 选择算子:个体存活的概率为: $P = f(x_i) / \sum f(x_i) $ 重组算子:两点交叉 变异:随机一位取反 确定最优属性:每次保留$y_i$个个体,n次运行后暴露的个体数为$\sum y_i$,统计每个属性在其中出现的频数,降序排列,即可得到最优属性 聚类 定义:将大量无标注的数据集,按数据的内在相似性将数据集划分成多个类别,使得类别内相似性较大,类别间相似性较小 对m个样本,构造k个簇,每个簇至少包含一个对象,每个对象仅属于一个簇 最短距离法: 定义距离计算方法 计算初始样本两两之间的距离,构造距离矩阵 根据距离矩阵,合并最小值对应得到两个样本,形成新样本 更新距离矩阵,重复上面的步骤 重心聚类法:计算每个簇的重心,重心可取样本的平均值,得到一个距离矩阵,后续同上 动态聚类法:先划分成若干个簇,计算所有样本到簇重心的距离,取最小距离作为该样本所属的簇,再更新簇的重心,直到所有的样本所属簇不再变化 k-means算法 对m个样本选取k个点作为中心,分别计算每个样本到这k个点的距离,并把该样本加入到最近距离的簇,所有样本加入到k个簇后,更新k个中心点,再重复,直至不再变化 k-means++算法 在k-means算法的基础之上,在初始生成k个初始点之前,对所有样本进行一次计算,使得初始中心点的距离尽可能远,这样可以减少迭代次数 先随机选择一个点作为第一个中心点,计算所有样本到该点的距离,选择距离最远的作为第二个样本点,依次选出k个 如何选择k值 k值越大,损失越小,k代表了模型的复杂度,模型越复杂,对训练集拟合的越好,但是引起过拟合 轮廓分析:$$s = \frac{b-a}{max(a,b)}$$其中a表示一个类别内所有数据到聚类中心的距离平方和,b表示数据到其他最近聚类中心的距离平方和,s=1时表示达到了最佳状态,s=-1表示最差状态,s越大越好 k-means总结 优点:当簇接近于高斯分布时效果较好;简单快捷; 缺点:必须先给出k值,对初始值敏感;不适合大小差别很大的簇;对噪声和孤立点敏感; LVQ(学习向量量化) 与k-means不同的是,假设样本带有标签 关键点在于中心点的更新,如果$x_i$与中心点的类别相同,则中心点向$x_i$靠拢,反之远离 密度聚类 初始化核心对象集合 计算每个样本的$\epsilon$-领域,将是核心对象的样本加入到集合中 以任何一个核心对象出发,找出由其密度可达的簇$C_i$ 直到集合内所有核心对象都被访问过 集成学习 Bagging 从D中创建T个训练子集 $D_1,D_2,D_3,…$ 为每一个$D_i$ 引入一个基学习器 $h_i$ 对回归问题使用简单平均;对分类问题使用简单投票 时间复杂度:$T(O(m)+O(n)$,$O(m)$是采样和投票的时间复杂度,$O(n)$是基学习器的计算复杂度 优点:泛化能力强;缺点:训练误差大 随机森林 以决策树为基学习器 分类问题,最终结果等于决策树输出次数最多的类别 回归问题,最终结果等于决策树输出结果的平均值 一颗决策树容易出错,但是多棵树同时犯错的概率就会很低 首先为每棵树创建训练子集;随机挑选部分属性构成候选属性子集,在这个子集中产生最优的属性划分;随机构成一个阈值集合 Schapire提升 bagging方法的严重缺陷在于各基学习器之间的关联性很弱 问题:用一个三元组和其他两个基分类器组成新的三元组后，集成分类器的性能就不会有多大的提升。 解决方法是三元组均由三个基分类器组成 Adaboosting 生成训练子集:生成第一个训练子集时,每个样本被选中的几率相同,并由此训练得到第一个分类器,然后减小被错误分类样本的概率,增大错分样本选中的概率 集成时,加大分类误差小的分类器的权重,减小分类误差大的分类器的权重 加法模型: $H(x) = \sum{a_ih_i(x)} $ 决策规则: $H(x) = sign( \sum{a_ih_i(x)})$ 贝叶斯 先验概率可以认为就是频率,后验概率就是条件概率,后验概率比先验概率更可信 贝叶斯公式:$$P(C_i|x) = \frac{P(x|C_i)*P(C_i)}{P(x)}$$ 贝叶斯假设,即属性之间是相互独立的:$$P(x|C_j) = \prod_{i=1}^dP(x_i|C_j)$$ 由此得到贝叶斯分类器:$$h(x) = argmax(P(C_j) * \prod_{i=1}^dP(x_i|C_j )$$ 属性之间不是相互独立的怎么办 忽略其影响 不能忽略时,可采用的方法有:去掉该属性,用其他属性替换之,或者采用别的分类方法 朴素贝叶斯算法 计算先验概率和条件概率,即各个标签的概率以及各个标签下各个特征的条件概率 对于给定的实例,计算$P(C_j) * \prod_{i=1}^dP(x_i|C_j )$ 选取其中的最大值,作为该实例的类别 平滑处理(拉普拉斯平滑) 先验概率,分母加上类别数,分子加1 条件概率,分母加上该属性的可能取值数,分子加一 KNN 三要素:k值选择,距离度量,分类决策规则 使用的距离:欧氏距离 k值得选择:k值减小意味着模型更复杂,容易过拟合;k值增加意味着模型变简单;实际k一般选一个较小的奇数,或者采用交叉验证的方式找到最好的k 分类决策规则:多数投票表决 kd树: 加权最近邻:距离近的权重大,距离远的权重小 具体的:将k个近邻点的距离从小到大排序:$d_1, d_2, d_3,…d_k$ 则$$\begin{cases}\frac{d_k-d_i}{d_k-d_1},d_k ≠ d_1\1, d_k=d_1\end{cases}$$ 标准化属性尺度:$x = (x - min)/(max-min)$ 线性回归 最小二乘法:基于方差最小化进行模型求解的方法就是最小二乘法 梯度下降 逻辑回归 sigmoid函数: $z = wx, f = \frac{1}{1+e^{-z}}, f’ = f*(1-f)$ 决策树 以信息熵为度量,构造一棵熵值下降最快的树,到叶子节点处的熵值为0 信息量 $I(x) = -log_2p(x)$, 事件发生的概率越小,信息量越大,对于确定性事件(p=1),信息量为0 熵是平均信息量:$H(x) = -\sum p(x)log_2p(x)$,则对于两点分布的来说:$H(x) = -p(x)log_2p(x)-(1-p(x))log(1-p(x))$ 条件熵:$H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)$ 熵和条件熵的概率值是由相对频率估算得到的话,叫做经验熵和条件经验熵 特征A对训练数据集D的信息增益:g(D,A) = H(D) - H(D|A) 经验熵$H(D)$表示对数据集D分类的不确定性,经验条件熵$H(D|A)$表示在特征A的给定条件下对数据集D分类的不确定性,他们的差值即信息增益表示特征A对数据集D的分类不确定性的减少程度,信息增益大的特征具有更强的分类能力 信息增益划分的缺点是太偏好取值数目较多的属性,C4.5采用信息增益率进行划分 信息增益率:$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$, 其中$H_A(D)$为特征A的经验熵 老师课件中提到:C4.5不是直接选取信息增益率最大的属性进行划分,而是从信息增益高于平均值属性中选取信息增益率最大的(统计学习方法一书中并没有这么说,就是C4.5根据信息增益率大小划分) 剪枝:为了降低过拟合,用叶子节点代替子树,会引起部分训练样例的错误分类,但是我们关心的是模型的泛化能力,而不是在训练集上的100%正确率 误差估计:错误率为$E=\frac{e+1}{n+m}$其中e是错误分类的样本数,n是达到测试节点t的样本总数,为避免n过小进行了修正 用测试样例比较剪枝前后的准确率,准确率提高则剪枝,反之不剪 预剪枝:在决策树生成的过程中,对每个节点划分前先进行估算,若划分后没有泛化能力的提升,则将该节点标记为叶子节点 后剪枝:先从训练集生成一棵完整的决策树,然后自底向上地对非叶子节点进行考察,若该节点换成叶子节点可以提高泛化能力,则剪枝 CART树 对回归树用平方误差最小化,对分类树用基尼指数最小化 分类树划分属性时采用的是基尼指数:$Gini(D) = \sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2$,对于二分类问题就是:$Gini(p)=2p(1-p)$ 若样本D根据特征A的取值被划分为D1和D2两部分,则在A条件下,D的基尼指数为:$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)$ 选择基尼指数最小的属性作为最优划分属性 人工神经网络 所谓的M-P模型就是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，它实际上就是对单个神经元的一种建模 BP算法的目标:最小化网络误差 解决过拟合: 采用早停策略,当训练误差降低而测试误差升高时,停止训练 正则化 跳出局部最小值: 随机梯度下降 以多种不同的参数初始化,训练后误差最小的作为最终结果 模拟退火,在每一步都以一定概率接受比当前解更差的结果]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法复习笔记]]></title>
    <url>%2F2019%2F01%2F25%2F%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[顺序表的静态定义12345678910111213// 第一种，定义一个数组int Sqlist[100];// 第二种，定义一个宏来描述大小#define List_size 100int Sqlist[List_size];// 第三种，定义一个结构体#define List_size 100typedef struct&#123; int elem[List_size]; int len; // 描述实际长度&#125;Sqlist; 顺序表动态定义1234567891011121314151617// 利用指针#define List_size 100typedef struct&#123; int *elem; int len; // 描述实际长度 int listsize; //当前分配空间大小&#125;Sqlist;int InitSqlist(Sqlist *L)&#123; L-&gt;elem = (int*)malloc(List_size * sizeof(int)); if(L-&gt;elem == NULL)&#123; exit(EXIT_FAILURE); &#125; L-&gt;len = 0; L-&gt;listsize = List_size; return 1;&#125; 顺序表插入操作12345678910111213141516// 在第i个位置插入eint listInsert_Sq(Sqlist &amp;L, int i, int e)&#123; // 插入位置合理 if(i&lt;1 || i&gt;L.len+1) return 0; // 出现上溢 if(L.len &gt;= L.listsize)&#123; newbase = (int*)realloc(L.elem, (L.listsize + ListIncerament) * sizeof(int)); if (newbase == NULL) exit(OVERFLOW); L.elem = newbase; L.listsize = L.listsize + ListIncerament; &#125; for(int j = L.len; j&gt;=i; j--) L.elem[j] = L.elem[j-1]; L.elem[i-1] = e; L.len++; return 1;&#125; 顺序表的删除操作1234567// 删除第i个元素int listDelete_Sq(Sqlist &amp;L, int i)&#123; if(i&lt;1 || i&gt; L.len) return 0; for(int j = i; j&lt;L.len; j++) L.elem[j-1] = L.elem[j]; L.len--; return 1;&#125; 顺序表优缺点 优点：随机存取 缺点：空间利用率低；静态的不可扩充；动态的反复扩充开销大；插入删除需要移动大量元素； 适用于输入数据大小已知，无需太多动态操作的应用 单链表的定义123456struct Node&#123; int data; struct Node *next;&#125;;typedef struct Node *Link;Link head; 有头节点的单链表插入算法1234567891011121314151617// 在第i个位置插入元素eint listInsert(Link &amp;L, int i, int e)&#123; Link p = L; int j = 0; while(p!= NULL &amp;&amp; j &lt; i-1)&#123; p = p-&gt;next; j++; &#125; if(p == NULL || j&gt;i-1) return 0; struct Node s; s = (Link)malloc(sizeof(Node)); if(s == NULL) exit(OVERFLOW); s-&gt;data = e; s-&gt;next = p-&gt;next; p-&gt;next = s; return 1;&#125; 无头节点的单链表插入算法12345678910111213141516171819202122int listInsert(Link &amp;L, int i, int e)&#123; if(i == 1)&#123; struct Node s; s = (Link)malloc(sizeof(Node)); s-&gt;data = e; s-&gt;next = L; L = s; &#125;else&#123; Link p = L; int j = 1; while(p != NULL &amp;&amp; j&lt;i-1)&#123; p = p-&gt;next; j++; &#125; if(p == NULL || j&gt;i-1) return 0; struct Node s; s = (Link)malloc(sizeof(Node)); s-&gt;data = e; s-&gt;next = p-&gt;next; p-&gt;next = s; &#125; return 1; 头插法1234s = (Link)malloc(sizeof(Node));scanf(&amp;s-&gt;data);s-&gt;next = L-&gt;next;L-&gt;next = s; 尾插法1234p-&gt;next = (Link)malloc(sizeof(Node));p = p-&gt;next;scanf(&amp;p-&gt;data);p-&gt;next = NULL; 双向链表定义12345678910struct Node&#123; int data; struct Node *next; struct Node *prev;&#125;;typedef struct Node *Link;typedef struct&#123; Link head, tail; int len;&#125;DLinkList; 顺序栈的定义12345678struct stack&#123; int *base; int stack_size; int min_stack; int max_stack; int top;&#125;;typedef struct stack Stack; 初始化栈12345678910111213Stack *CreateStack(int how_many)&#123; Stack *pstk; assert(how_many&gt;0); pstk = (Stack*)malloc(sizeof(Stack)); if(pstk == NULL) return(NULL); pstk-&gt;stack_size = how_many; pstk-&gt;base = (int*)malloc(how_many * sizeof(int)); if(pstk-&gt;base == NULL) return(NULL); pstk-&gt;min_stack = 0; pstk-&gt;max_stack = how_many-1; pstk-&gt;top = -1; return(pstk);&#125; 销毁栈12345void DestroyStack(Stack *this_stack)&#123; this_stack-&gt;top = -1; free(this_stack-&gt;base); this_stack-&gt;base = NULL;&#125; 随机访问栈元素123456// which_elem 表示待查元素与栈顶的间隔int* viewElem(Stack *this_stack, int which_elem)&#123; if(this_stack-&gt;top == -1) return(NULL); if(this_stack-&gt;top - whick_elem &lt; 0 ) return(NULL); return(&amp;(this_stack-&gt;base[this_stack-&gt;top-which_elem]));&#125; 入栈1234567int pushElem(Stack *this_stack, int *to_push)&#123; // 判断栈满 if(this_stack-&gt;top == this_stack-&gt;max_stack) return(0); this_stack-&gt;top += 1; memmove(&amp;(this_stack-&gt;base[this_stack-&gt;top]), to_push, sizeof(int)); return(1);&#125; 出栈1234567int popElem(Stack *this_stack, int *dest)&#123; // 判断栈空 if(this_stack-&gt;top == -1) return(0); memmove(dest, &amp;(this_stack-&gt;base[this_stack-&gt;top]), sizeof(int)); this_stack-&gt;top -= 1; return(1);&#125; 推荐适用顺序栈 实现简单 栈的受限操作正好屏蔽了顺序表的弱势：插入和删除都是在同一端进行的 栈、队列对比 栈：栈顶（top）允许插入和删除的一端；栈底（base/bottom）表头端 队列：队头（front）允许删除的一端；队尾（rear）允许插入的一端 队列的应用：操作系统的作业排队顺序队列定义1234567#define Max_QSize 100typedef struct&#123; int *base; int front; int rear;&#125; queue_struct;queue_struct SqQueue; 顺序对列的入队出队以及队空队满 入队：若未满，Q.rear++ 出队：若未空，Q.front++ 队空：Q.front == Q.rear 队满：Q.rear == Max_QSize 问题是存在假上溢 构建循环队列，解决假上溢的问题 入队：若未满，Q.rear = (Q.rear+1)% Max_QSize 出队：若不空，Q.front = (Q.front+1)% Max_QSize 队空：Q.rear == Q.front 队满：Q.rear == Q.front 问题是对空队满判断一样 解决：少一个元素空间，堆满改为：(Q.rear+1)% Max_QSize == Q.front 链队列 队头用链表的头指针表示 队尾用链表的尾指针表示 无队满问题 队空：Q.rear == Q.front Hash表定义 用动态定义的顺序表来定义hash表 1234567#define Table_Size 100struct HashTable_struct&#123; int *elem; int len; int tableSize;&#125;;typedef struct HashTable_struct hash_table; 冲突处理办法 线性再散列 非线性再散列 外部拉链法 线性再散列的缺点是1.不能删除表中元素，解决办法是把用过的槽标记为无效2.当表被填满时性能显著下降 再散列法的优点是1容易进行动态编码2负载因子较低且不太可能删除元素时速度快，负载因子大于0.5时不建议使用 外部拉链法可以容纳的元素取决于内存的大小，而再散列法取决于表的大小 外部拉链法平均查找时间 = 链表长度/2 + 1 外部拉链法的缺点是需要更多的存储空间 二分查找123456789101112131415161718192021222324252627// 确定在有序整数序列x中，整数t第一次出现的位置int binarySearch(int t)&#123; int l,u,m; l = 0; u = n-1; for(;;)&#123; if(l&gt;u) return -1; m = (l+u)/2; if(x[m] &lt; t) l = m+1; else if(x[m] == t) return m; else u = m-1; &#125;&#125;// 优化1int binarySearch1(int t)&#123; int l,u,m; l = 0; u = n-1; while(l&lt;=u)&#123; m = (l+u)/2; if(x[m]&lt;t) l = m+1; else if(x[m] == t) return m; else u = m-1; &#125; return -1;&#125; 代码调优通用法则 利用等价的代数表达式，比如模运算的优化 利用宏替换函数，但有时会起反作用 利用哨兵合并测试条件 展开循环 高速缓存需经常处理的数据 1234567891011121314151617// 整数取模// 优化前k = (j + r) % n;// 优化后k = j + r;while(k &gt; n) k -= n; // 利用宏// 优化前float max(float a, float b)&#123; return a&gt;b?a:b;&#125;// 优化后#define max(a,b) ((a)&gt;(b)?(a):(b)); 朴素的模式匹配算法 时间复杂度：设n为主串长度，m为子串长度，O((n-m+1)*m) 最好情况：O(m) 平均情况：每次匹配不成功都是在子串的首字符处，O(m+n) 最坏情况：O((n-m+1)*m) 缺点是需要回溯 KMP算法 近似时间复杂度为O(m+n), O(n)为比较的时间，O(m)为计算next数组的时间 目标串中不存在回溯 目标串中每个字符会比较1～2次 字符串的下标是从1开始 仅当模式串与目标串存在许多部分匹配时，KMP才比朴素模式匹配有优势 若每一次不匹配都发生在第一个字符，KMP会退化为朴素模式匹配 BM算法 从右往左 坏字符：子串从右开始标，第一个为0，每种字符只标一次，大小为到最右的距离，其他为整个子串长度 好后缀：子串从右开始标，第一个为1，完整的好后缀从右往左找重复的，不完整的好后缀从头往后找重复的，大小等于好后缀长度加上到重复的距离 通常模式串越长，BM算法越快，因为每一次失败的匹配，BM都能够使用这些信息来排除尽可能多的无法匹配的位置 二叉树的性质 第i层最多 $2^{i-1}$ 个节点 深度为k的二叉树最多 $2^{k+1}-1$个节点 $n_0 = n_2 + 1$ 二叉树顺序存储定义12345678910// 定义1#define MAX_TREE_SIZE 100ElemType SqBiTree[MAX_TREE_SIZE];// 定义2#define MAX_TREE_SIZE 100typedef struct&#123; ElemType elem[MAX_TREE_SIZE+1]; int len;&#125;SqBiTree; 二叉链表定义1234typedef struct&#123; ElemType data; struct BiTNode *lchild, *rchild;&#125;BiTNode, *BiTree; 二叉排序树 中序遍历二叉排序树，可以得到一个关键字的有序序列 删除操作：用中序遍历的直接后继或直接前驱进行替换 构造二叉排序树的形状依赖于数据项，且依赖于它们加载的顺序 为解决二叉排序树的失衡问题，提出了AVL树，红黑树，伸展树 AVL树 发生失衡时，找到不平衡的最小子树根结点开始调节 红黑树 2-3-4树 插入时，遇到4节点就要分 伸展树 优点：最近使用过的数据比未使用的数据更快被访问 查找：若n在树t上，则以n为根结点进行重排；若n不在树t上，则将查找n的过程中第一个非空叶子节点作为根结点进行重排；最后判断根结点与n是否相同 插入：根据重排后的根结点与待插入的n之间的关系，决定n插入的位置 删除：重排后，待删除的n位于根节点处，删除n后，若左孩子非空，则用中序遍历的前驱代替n，若左孩子为空，则用右孩子代替 自底向上：待访问节点c，父节点p，祖父节点g 若c无祖父，直接在cp之间旋转 若cpg之间为LL或RR的关系，则pg先旋转，然后pc旋转 若cpg之间为LR或RL的关系，则cp先旋转，然后cg旋转 B树/B+树 如何快速检索内存中的数据 二叉排序树，hash表，二分查找 如何快速检索磁盘上的海量数据 减少磁盘IO次数和提高内存检索效率 B和B+区别 B+的叶子节点包含了全部关键字信息以及指向这些关键字的指针，叶子节点本身也以关键字大小自小而大顺序连接 所有非终端节点可以看做索引部分，节点中含有其子树最大或最小的关键字 B+树的特点 B+树不是树 两部分组成 索引块，指针指向块号 数据块，指针指向记录地址 B+树的查找 顺序查找（横向） 随机查找（纵向） B+树的查询效率是常数，B+树的查询效率对于某建成的树是固定的，而B树是与键在树中的位置有关的 应用场景：磁盘上海量数据的检索 Trie树 特性： 1 根节点不含字符，根节点外每个节点含有一个字符 2 从根节点到某一节点路径上经过的字符连接起来为该节点对应的字符串 3 每个节点的子节点都不相同 12345678// Trie的定义const int kind = 26typedef struct Treenode&#123; int count; bool isColored; TrieNode *next[kind];&#125;TrieNode;TrieNode *root; 典型应用：用于统计和排序大量的字符串，所以经常被搜索引擎用于文本词频统计 查找某个字符串操作的复杂度为O(N),其中N为字符串长度，空间换时间 优点：利用字符串的公共前缀来节省存储空间，最大限度减少无谓的比较，查询效率比hash高 中文字典树：Trie树的每个子节点用hash存储 排序 通常排序算法都设计为用于数组，非常适合链表的排序方法有：插入排序和快排 内部排序：把待排的所有记录都加载进内存，然后对内存中的这些记录进行排序 外部排序：待排序的记录无法一次性加载到内存中，因此每次加载部分记录，并进行内部排序 基于比较的排序算法：平均复杂度≥O(NlogN) 交换：冒泡，快排 选择：简单选择，堆排序 插入：直接插入，折半插入，希尔排序 基于某种映射的排序，平均时间复杂度为线性级别：桶排序，基数排序 冒泡 连续扫描待排序的记录 每趟扫描都把最大的记录移动到序列的尾部 若某趟扫描没有任何交换，则表明是有序的了 时间复杂度：平均：O($N^2$),最坏：O($N^2$),最好：O(N) 空间复杂度：交换时需要一个辅助空间temp，所以是O(1) 基于数组的冒泡排序的基本操作是比较和交换 优点是：简单，容易实现；对几乎有序的记录排序的时间开销为O(N) 由相邻的记录进行交换，所有是稳定的 12345678910111213141516171819// 外层循环控制执行的趟数，内层循环控制在一趟中相邻记录间的比较和交换void BubbleSort(Element **Array, int N, CompFunc Compare)&#123; int limit; for(limit = N-1; limit&gt;0; limit--)&#123; int j, swapped; swapped = 0; for(j = 0; j&lt;limit; j++)&#123; if(Compare(Array[j], Array[j+1])&gt;0)&#123; Element *temp; temp = Array[j]; Array[j] = Array[j+1]; Array[j+1] = temp; swapped = 1 &#125; &#125; if(!swapped) break; &#125;&#125; 简单选择排序 连续扫描序列A，不断从待排记录中选出最小的记录放到已排序记录序列的后面，直到n个记录全部插入到已排序序列中 不稳定的：比如3，3，3**，2，在第一遍过后就会变为2，3，3** 对于长度是N的数组，选择排序需要大约N2/2次比较和N次交换 总的来说时间复杂度是T(N) = O($N^2$) 空间复杂度是：交换时需要辅助空间temp，O(1) 12345678910111213141516171819// 外循环用于控制排序的次数，内循环用于查找待排记录中关键字最小的记录void SelectSort(Element **Array, int N, CompFunc Compare)&#123; int i, j, cnt; for(i = 0; i&lt;N-1; i++)&#123; Element *temp; temp = Array[i]; cnt = i; for(j = i+1; i&lt;N; j++ )&#123; if(Compare(temp, Array[j])&gt;0)&#123; temp = Array[j]; cnt = j; &#125; &#125; if(cnt != i)&#123; Array[cnt] = Array[i]; Array[i] = temp; &#125; &#125;&#125; 直接插入排序 逐个处理待排序的记录，将每个记录与前面已经排好的记录序列进行比较，并将其插入到合适的位置 对于已经排序好的序列，从后往前进行扫描 空间复杂度：只需要一个temp，所以O(1) 稳定性：稳定 时间复杂度：外循环始终n-1次，最好情况即为初始是正序的，时间复杂度为：O(n),最坏情况为逆序序列，时间复杂度为O($N^2$),平均情况为O($N^2$) 越接近于有序，该算法效率越高 基于数组的操作：比较和半交换(移位) 优点：对几乎有序的时间开销为O(n);可以用于优化快排 改进：折半插入排序，直接插入排序的查找插入位置采用折半查找的方法实现 12345678910111213141516void InsertSort(Element **Array, int N, CompFunc Compare)&#123; int step; for(step = 1; step&lt;N; step++)&#123; int i; Element *temp; temp = Array[step]; for(i = step-1; i&gt;=0; i--)&#123; if(Compare(Array[i], temp)&gt;0)&#123; Array[i+1] = Array[i]]; &#125; else break; &#125; Array[i+1] = temp; &#125;&#125; 希尔排序 直接插入的改进版 直接插入排序的问题：每次扫描序列，智能确定一个目标的合法位置 思想：先分割成若干小组，分别在组内进行直接插入排序，待基本有序后，再对全体进行一次直接插入排序，每个组内记录间隔h，h稳定递减，h最后一个取值为1 改进的出发点是：直接插入排序对几乎有序的记录排序时间开销为O(N) 时间复杂度为：O($n^{1.25}$) 不稳定 1234567891011121314151617181920void ShellSort(Element **Array, int N, CompFunc Compare)&#123; int step, h; for(h = 1; h&lt;= N/9; h = 3*h+1) ; // h = 1,4,13,40... for(;h&gt;0;h/=3)&#123; //h = ...,40,13,4,1 for(step = h; step&lt;N; step++)&#123; int i; Element *temp; temp = Array[step]; for(i = step-h; i&gt;0; i-=h)&#123; if(Compare(temp, Array[i])&lt;0)&#123; Array[i+h] = Array[i] &#125; else break; &#125; Array[i+h] = temp; &#125; &#125;&#125; 快速排序 分治法:将待排数组分成两个小部分,分别进行递归快排 思想: 若待排的数组中只有一个元素,则退出 否则选择一个元素作为基准 将待排的数组按该元素划分成两个数组A1和A2,其中A1中的元素都小于等于该元素,A2中的元素都大于等于该元素 对A1进行快排 对A2进行快排 12345678910111213141516171819// 算法实现1// 单向划分 &amp; 将数组的首元素作为基准// 性能分析:对于随机数组:T(n) = O(nlogn),栈深度:O(logn)// 因为每层递归都执行了O(n)次比较,总计O(logn)次递归// 对于非随机数组:不管是相同的元素还是升降序的,T(n2)void qsort1(int l, int u)&#123; int i,m; if(l&gt;=u) return; m = l; // m表示存放小于基准元素的子数组A1的最末元素的下标,A1采用尾插法来添加新的元素 for(i = l+1; i&lt;=u; i++)&#123; if(x[i]&lt;x[l])&#123; swap(++m, i); &#125; &#125; swap(l, m); qsort1(l, m-1); qsort1(m+1, u);&#125; 12345678910111213141516171819202122232425// 算法实现2// 双向划分 &amp; 将数组首元素作为基准// i右移过小元素,遇到大元素时停止// j左移过大元素,遇到小元素时停止// 若i,j不交叉,则交换两下标对应的元素// 性能分析:对于随机顺序的数组:T(n) = O(nlogn),栈深度:O(logn)// 对于非随机的数组:若果是相同的元素:T(n) = O(nlogn);如果是升降序的:T(n)=O(n2)void qsort3(int l, int u)&#123; int i, j; DType t; if(l &gt;= u) return; t = x[l]; i = l; j = u+1; for(;;)&#123; do i++;while(i&lt;=u &amp;&amp; x[i]&lt;t); do j--;while(x[j]&gt;t); // 这里不使用等号是怕遇到所有元素都相同时,时间复杂度降到O(n2) if(i&gt;j) break; swap(i, j); &#125; swap(l, j); qsort3(l, j-1); qsort3(j+1, u);&#125; 1234567891011121314151617181920212223// 双向划分 &amp; 将随机元素作为基准// 优点是对于小的子数组,可采用插入排序的方法void qsort4(int l , int u)&#123; int i, j; DType t, temp; if(u-l &lt; cutoff) return; swap(l, randint(l,u)); t = x[l]; i = l; j = u+1; for(;;)&#123; do i++; while(i&lt;=u &amp;&amp; x[i]&lt;t); do j--; while(x[j]&gt;t); if(i&gt;j) break; temp = x[i]; x[i] = x[j]; x[j] = temp; &#125; swap(l, j); qsort4(l, j-1); qsort4(j+1, u);&#125; 其他改进提速方法 改进栈利用率,大的子数组改用迭代循环,小的子数组仍递归快排 利用static,减少栈空间需求 不使用显式的数组索引,改用等价的指针,以保证快排的稳定性 总结 系统自带的sort函数能满足需求,则不需要编写代码 元素个数较少时,可以考虑直接插入排序 元素个数较大时,可以考虑编码实现的快排 堆排序 不存在最坏情况 T= O(nlogn) 和希尔排序一样,是良好的通用排序算法 siftup(n) 在堆的尾部插入新元素后,需要重新获取堆的性质 siftdown(1, n) 用新元素替换堆中的根后,需要重新获取堆的性质 两个阶段: 建立大根堆 依次提取大根堆的根节点,从左到右建立最终的升序序列 12345678910111213141516171819202122232425262728293031323334void hsort1()&#123; int i; x--; for(i = 2; i&lt;=n; i++) siftup(i); for(i = n; i&gt;=2; i--)&#123; swap(1, i); siftdown1(1, i-1); &#125; x++;&#125;void siftup(int u)&#123; int i, p; i = u; for(;;)&#123; if(i == 1) break; p = i/2; if(x[p]&gt;=x[i]) break; swap(p, i); i = p; &#125;&#125;void siftdown1(int l, int u)&#123; int i,c; i = l; for(;;)&#123; c = 2 * i; if(c&gt;u) break; if(c+1 &lt;= u &amp;&amp; x[c+1]&gt;x[c]) c++; if(x[i]&gt;x[c]) break; swap(i,c); i =c; &#125;&#125; 后缀数组 将文本中所有字符保存到字符数组和后缀数组中 对后缀数组进行排序,将相似的元素聚集在一起 扫描后缀数组,比较相邻后缀数组中的相邻元素,找出最长的重复字符串 12345678910111213141516171819202122232425262728int main()&#123; int i, ch,n=0,maxi,maxlen=-1; // a[n]为后缀数组,c[n]为字符数组 while((ch = getchar())!= EOF)&#123; a[n] = &amp;c[n]; c[n++] = ch; &#125; c[n] = 0; // 按字符重新排序 qsort(a,n,sizeof(char*),pstrcmp); // 统计最长重复子串 for(i=0;i&lt;n-1;i++) if(comlen(a[i], a[i+1])&gt;maxlen)&#123; maxlen = comlen(a[i], a[i+1]); maxi = i; &#125; printf("%.*s\n", maxlen, a[maxi]); return 0;&#125;int pstrcmp(char **p, char **q)&#123; return strcmp(*p, *q);&#125;int comlen(char *p, char *q)&#123; int i=0; while(*p &amp;&amp; (*p++ == *q++)) i++; return i;&#125; 海量数据的处理统计 双层桶划分+Trie树/hash表/红黑树 将大文件划分成若干个小文件,利用hash技术 对每个小文件进行词频统计 合并各个小文件的结果 云计算架构 线性结构+直接排序法 顺序表存储,归并排序进行排序,拍完再遍历统计词频 理论上排序的时间复杂度是O(nlogn),遍历的时间复杂度是O(n),所以总体的是时间复杂度是O(nlogn) 实际上每个 单词不止I/O一次,I/O属于耗时操作,所以实际上的时间复杂度远比理论上的大 总结: 如果海量数据无法一次性在内存中处理,可以划分成多个可以在内存中处理的数据区域 再用Trie树/hash表统计 排序 顺序表+直接排序法 先外部排序,比如归并排序,时间复杂度O(nlogn) 对排序完的进行遍历,统计次数,O(n) 总体的时间复杂度就是O(nlogn) 利用hash表 key为字串,value为出现次数 时间复杂度为:O(n * 每个槽上的线性表平均长度) 相比较上一个,不仅仅时间复杂度优化,而且只要IO一次,更好 找出TOP10:可采用内部排序,局部淘汰,堆 关于重复项的处理 分而治之+基于hash表的查找 分别遍历a,b,求hash,分到若干个小文件中 对每个小文件,将a中的数据存储到hash中,遍历b中每个数据,查询是否在hash中 若允许有一定的错误率,可采用bloom filter 是位图法的扩展 k个hash函数,每个字符串与k个bit对应,k越大,冲突的概率就越小 存在查询结果的误判,但是节省了存储开销 无需处理碰撞 $k=(ln2)*(m/n)$ 时的错误率是最小的,其中n为元素个数,m为位数组的大小,m的单位是bit,n的单位是个数,通常单个元素的长度都是狠多bit的,所以bloom filter是可以节省空间的 在错误率不大于E的情况下,$m /geq nlog_2(1/E)log_2e = nlog_2(1/E)*1.44$ 缺点:不可逆,无法恢复表示的数据,因为hash表不可逆;不能删除元素]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
</search>
