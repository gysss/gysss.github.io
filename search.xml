<?xml version="1.0" encoding="utf-8"?>
<search>
  <entry>
    <title><![CDATA[五天数据清理挑战]]></title>
    <url>%2F2019%2F03%2F01%2F%E4%BA%94%E5%A4%A9%E6%95%B0%E6%8D%AE%E6%B8%85%E7%90%86%E6%8C%91%E6%88%98%2F</url>
    <content type="text"><![CDATA[具体内容来源及文件下载请到：Data Cleaning Challenge Day1:Handling missing valuesTake a first look at the data导入库和读取csv文件不再赘述！首先，检查是否有缺失值 12# 查看5行数据nfl_data.sample(5) See how many missing data points we have检查各个column有多少缺失值 12345# get the number of missing data points per columnmissing_values_count = nfl_data.isnull().sum()# look at the # of missing points in the first ten columnsmissing_values_count[0:10] 查看缺失值的百分比,这样或许更加直观 123456# how many total missing values do we have?total_cells = np.product(nfl_data.shape) # np.product:Return the product of array elements over a given axistotal_missing = missing_values_count.sum()# percent of data that is missing(total_missing/total_cells) * 100 Figure out why the data is missing一个很重要的问题是： 某个数据的缺失是因为它没有被记录还是压根就不存在 如果是本来就不存在的话，那就没必要去猜它是啥了如果是因为没有被记录的话，那么就需要基于这个数字所在的行和列的其他数据来猜它了 Tips：反复阅读数据的文档说明是个很好的方法 Drop missing values如果说处理数据很匆忙，或者无法弄明白为啥缺失了，可以选择移除包含该数据的行或者列（但是，不推荐这样做）移除带有缺失值的行： 12# remove all the rows that contain a missing valuenfl_data.dropna() 移除带有缺失值的列： 1columns_with_na_dropped = nfl_data.dropna(axis=1) Filling in missing values automatically除了直接移除缺失值外，也可以尝试来填补首先是直接用0来填补 12345# get a small subset of the NFL datasetsubset_nfl_data = nfl_data.loc[:, 'EPA':'Season'].head()# replace all NA's with 0subset_nfl_data.fillna(0) 也可以通过设置method = ‘bfill’，来用相邻行的同column数据来填补 123# replace all NA's the value that comes directly after it in the same column, # then replace all the reamining na's with 0subset_nfl_data.fillna(method = 'bfill', axis=0).fillna(0) Day2:Scaling and normalizationScalingScaling:缩放，修改数据的范围，常用于SVM和KNN 123456789101112# generate 1000 data points randomly drawn from an exponential distributionoriginal_data = np.random.exponential(size = 1000) # 绘制指数分布# mix-max scale the data between 0 and 1scaled_data = minmax_scaling(original_data, columns = [0])# plot both together to comparefig, ax=plt.subplots(1,2)sns.distplot(original_data, ax=ax[0])ax[0].set_title("Original Data")sns.distplot(scaled_data, ax=ax[1])ax[1].set_title("Scaled data") 可以看到数据的形状并没有变化，但范围是从0~8变为了0~1 NormalizationNormalization：标准化，使数据尽可能满足正态分布，常用于t-tests, ANOVAs, linear regression, linear discriminant analysis (LDA) and Gaussian naive Bayes这里使用的是Box-Cox转化 123456789# normalize the exponential data with boxcoxnormalized_data = stats.boxcox(original_data)# plot both together to comparefig, ax=plt.subplots(1,2)sns.distplot(original_data, ax=ax[0])ax[0].set_title("Original Data")sns.distplot(normalized_data[0], ax=ax[1])ax[1].set_title("Normalized data") 可以看到转化后更加接近于正态分布了 Day 3: Parsing datesCheck the data type of our date column1print(landslides['date'].head()) 输出： 0 3/2/071 3/22/072 4/6/073 4/14/074 4/15/07Name: date, dtype: object 作为人，可以清楚地知道这些数代表的是日期，但是Python不知道，左下角的dtpe：object，Pandas使用object表示数据包含多种数据种类，但通常都包含字符串。其实Pandas是有一种datetime64的数据类型的 12# check the data type of our date columnlandslides['date'].dtype 输出： dtype(‘O’) 同样也可以用这种方法查看数据类型，‘O’表示object Convert our date columns to datetime将字符串转化为 datetime 时，要注明原字符串在表示日期时所遵循的格式，%d for day, %m for month, %y for a two-digit year and %Y for a four digit year.具体的： 1/17/07 has the format “%m/%d/%y” 17-1-2007 has the format “%d-%m-%Y” 12landslides.date = pd.to_datetime(landslides.date, format="%m/%d/%y")landslides.date.head() 输出： 0 2007-03-021 2007-03-222 2007-04-063 2007-04-144 2007-04-15Name: date, dtype: datetime64[ns] 如果日期的格式不是固定的”%m/%d/%y”，而是多种格式的混合时，可以让pandas来推断正确的格式，这样： 1landslides['date_parsed'] = pd.to_datetime(landslides['Date'], infer_datetime_format=True) 但是不要总是使用这个方法，因为pandas不能总是判断正确格式，而且会很慢 Select just the day of the month from our column对于datatime的数据，可以选取出其中的day，month，year 123day_date = landslides.date.dt.daymonth_date = landslides.date.dt.monthyear_date = landslides.date.dt.year Plot the day of the month to check the date parsing可视化日期 12345# remove na'sday_date = day_date.dropna()# plot the day of the monthsns.distplot(day_date, kde=False, bins=31) # kde 用于调节是否显示核密度估计 bins 用于控制直方图的划分 补充一个时间序列可视化教程 Day 4: Character encodingsWhat are encodings?在Python3处理文本时，可能会遭遇的两种数据类型，一种是string，文本的默认属性 12345# start with a stringbefore = "This is the euro symbol: €"# check to see what datatype it istype(before) 输出： str 另一种是bytes，是一个整数序列，可以从string转成bytes 12345# encode it to a different encoding, replacing characters that raise errorsafter = before.encode("utf-8", errors = "replace")# check the typetype(after) 输出： bytes 当然也可以从bytes转回成string 1after.decode('utf-8') 输出： ‘This is the euro symbol: €’ Reading in files with encoding problems读取非UTF-8编码的文件会出错，可以采用的方法是先读取前10000bytes来判断下 123456# look at the first ten thousand bytes to guess the character encodingwith open("../input/kickstarter-projects/ks-projects-201801.csv", 'rb') as rawdata: result = chardet.detect(rawdata.read(10000))# check what the character encoding might beprint(result) 输出： {‘encoding’: ‘Windows-1252’, ‘confidence’: 0.73, ‘language’: ‘’} 即chardet有73%概率认为是Windows-1252编码的，然后可以按此编码方式来打开文件 12345# read in the file with the encoding detected by chardetkickstarter_2016 = pd.read_csv("../input/kickstarter-projects/ks-projects-201612.csv", encoding='Windows-1252')# look at the first few lineskickstarter_2016.head() Saving your files with UTF-8 encoding默认都是按utf-8来保存的 12# save our file (will be saved as UTF-8 by default!)kickstarter_2016.to_csv("ks-projects-201801-utf8.csv") Day 5: Inconsistent Data EntryDo some preliminary text pre-processing首先是检查city这一栏，有没有输入错误 123456# get all the unique values in the 'City' columncities = suicide_attacks['City'].unique()# sort them alphabetically and then take a closer lookcities.sort()cities 通过全部转小写，去头尾的空格，可以消除80%的输出错误 1234# convert to lower casesuicide_attacks['City'] = suicide_attacks['City'].str.lower()# remove trailing white spacessuicide_attacks['City'] = suicide_attacks['City'].str.strip() Use fuzzy matching to correct inconsistent data entry使用fuzzywuzzy包， 可以用来分辨那些相似的字符串Fuzzywuzzy返回的是两个字符串之间的几率值，越接近于100，相似度也就越高 12345# get the top 10 closest matches to "d.i khan"matches = fuzzywuzzy.process.extract("d.i khan", cities, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio)# take a look at themmatches 输出： [(‘d. i khan’, 100), (‘d.i khan’, 100), (‘d.g khan’, 88), (‘khanewal’, 50), (‘sudhanoti’, 47), (‘hangu’, 46), (‘kohat’, 46), (‘dara adam khel’, 45), (‘chaman’, 43), (‘mardan’, 43)] 可以针对几率在90以上的进行替换，因此可以写出个转化的函数 123456789101112131415161718192021# function to replace rows in the provided column of the provided dataframe# that match the provided string above the provided ratio with the provided stringdef replace_matches_in_column(df, column, string_to_match, min_ratio = 90): # get a list of unique strings strings = df[column].unique() # get the top 10 closest matches to our input string matches = fuzzywuzzy.process.extract(string_to_match, strings, limit=10, scorer=fuzzywuzzy.fuzz.token_sort_ratio) # only get matches with a ratio &gt; 90 close_matches = [matches[0] for matches in matches if matches[1] &gt;= min_ratio] # get the rows of all the close matches in our dataframe rows_with_matches = df[column].isin(close_matches) # replace all rows with close matches with the input matches df.loc[rows_with_matches, column] = string_to_match # let us know the function's done print("All done!") 然后便可以针对某一个字符串进行相似词的替换了 12# use the function we just wrote to replace close matches to "d.i khan" with "d.i khan"replace_matches_in_column(df=suicide_attacks, column='City', string_to_match="d.i khan")]]></content>
      <tags>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Kaggle实战之Titanic]]></title>
    <url>%2F2019%2F02%2F27%2FKaggle%20%E4%B9%8B%20Titanic%2F</url>
    <content type="text"><![CDATA[题目来自：Titanic参考资料来自：An Interactive Data Science TutorialTitanic 生存预测比赛是一个二分类问题，根据乘客的信息来判断是否在沉船事故中存活了下来。 首先还是导入必要的库： 1234567891011121314151617181920212223242526272829303132# Ignore warningsimport warningswarnings.filterwarnings('ignore')# Handle table-like data and matricesimport numpy as npimport pandas as pd# Modelling Algorithmsfrom sklearn.tree import DecisionTreeClassifierfrom sklearn.linear_model import LogisticRegressionfrom sklearn.neighbors import KNeighborsClassifierfrom sklearn.naive_bayes import GaussianNBfrom sklearn.svm import SVC, LinearSVCfrom sklearn.ensemble import RandomForestClassifier , GradientBoostingClassifier# Modelling Helpersfrom sklearn.preprocessing import Imputer , Normalizer , scalefrom sklearn.model_selection import train_test_split , StratifiedKFoldfrom sklearn.feature_selection import RFECV# Visualisationimport matplotlib as mplimport matplotlib.pyplot as pltimport matplotlib.pylab as pylabimport seaborn as sns# Configure visualisations%matplotlib inlinempl.style.use( 'ggplot' )sns.set_style( 'white' )pylab.rcParams[ 'figure.figsize' ] = 8 , 6 其次是一些用于绘图的功能性函数： 12345678910111213141516171819202122232425262728293031323334353637383940414243444546474849505152535455565758596061626364def plot_histograms( df , variables , n_rows , n_cols ): fig = plt.figure( figsize = ( 16 , 12 ) ) for i, var_name in enumerate( variables ): ax=fig.add_subplot( n_rows , n_cols , i+1 ) df[ var_name ].hist( bins=10 , ax=ax ) ax.set_title( 'Skew: ' + str( round( float( df[ var_name ].skew() ) , ) ) ) # + ' ' + var_name ) #var_name+" Distribution") ax.set_xticklabels( [] , visible=False ) ax.set_yticklabels( [] , visible=False ) fig.tight_layout() # Improves appearance a bit. plt.show()def plot_distribution( df , var , target , **kwargs ): row = kwargs.get( 'row' , None ) col = kwargs.get( 'col' , None ) facet = sns.FacetGrid( df , hue=target , aspect=4 , row = row , col = col ) facet.map( sns.kdeplot , var , shade= True ) facet.set( xlim=( 0 , df[ var ].max() ) ) facet.add_legend()def plot_categories( df , cat , target , **kwargs ): row = kwargs.get( 'row' , None ) col = kwargs.get( 'col' , None ) facet = sns.FacetGrid( df , row = row , col = col ) facet.map( sns.barplot , cat , target ) facet.add_legend()def plot_correlation_map( df ): corr = titanic.corr() _ , ax = plt.subplots( figsize =( 12 , 10 ) ) cmap = sns.diverging_palette( 220 , 10 , as_cmap = True ) _ = sns.heatmap( corr, cmap = cmap, square=True, cbar_kws=&#123; 'shrink' : .9 &#125;, ax=ax, annot = True, annot_kws = &#123; 'fontsize' : 12 &#125; )def describe_more( df ): var = [] ; l = [] ; t = [] for x in df: var.append( x ) l.append( len( pd.value_counts( df[ x ] ) ) ) t.append( df[ x ].dtypes ) levels = pd.DataFrame( &#123; 'Variable' : var , 'Levels' : l , 'Datatype' : t &#125; ) levels.sort_values( by = 'Levels' , inplace = True ) return levelsdef plot_variable_importance( X , y ): tree = DecisionTreeClassifier( random_state = 99 ) tree.fit( X , y ) plot_model_var_imp( tree , X , y ) def plot_model_var_imp( model , X , y ): imp = pd.DataFrame( model.feature_importances_ , columns = [ 'Importance' ] , index = X.columns ) imp = imp.sort_values( [ 'Importance' ] , ascending = True ) imp[ : 10 ].plot( kind = 'barh' ) print (model.score( X , y )) 训练集与测试集接下来就是导入训练集和测试集了，以及对两个数据集进行了合并，以便于后面进行数据分析，特征工程等： 12345train_data = pd.read_csv('train.csv')test_data = pd.read_csv('test.csv')full = train_data.append(test_data, ignore_index=True)titanic = full[:891] # full是整个数据集，titanic是训练集print('full:', full.shape, ';titanic:', titanic.shape) 输出： full: (1309, 12) ;titanic: (891, 12) 数据分析使用full.head()可以查看前几个数据的样式，如下所示：使用titanic.info可以获取训练集每个column数据信息：使用test_data.info可以获取测试集每个column数据信息:关于各个column的信息如下：Age：年龄，有中等数量的缺失cabin：座舱号，有大量的缺失Embarked：登船口，在训练集中有很少量的缺失(2个)，包括C，Q，S三种Fare：乘客的票价，在测试集中有很少量的缺失(1个)Name：姓名Parch：乘客的父母和孩子的个数PassengerId: 自增数值，无意义Pclass：票的等级，有三级：1，2，3Sex：性别，male和femaleSibSp：乘客的兄弟和配偶的个数Survived：是否存活，0 = No, 1 = Yesticket：票的编号 绘制相关性的heat map，可能可以知道哪些变量是很重要的 1plot_correlation_map(titanic) 输出如下图：接下来绘制一些特征与存活与否之间的关系首先是Age，Sex与Survived关系图： 1plot_distribution(titanic , var = 'Age' , target = 'Survived' , row = 'Sex') 输出如下图：两个线差别较大的地方，代表了更好的区分度。可以看到年龄小的男性更多的存活，中等年龄的男性更多的死亡 Fare和Survived的关系图： 1plot_distribution(titanic , var = 'Fare' , target = 'Survived' ) 输出如下图：可以看到，低票价有着更高的死亡率 接下来看Embarked与Survived的关系： 12print(titanic.Embarked.value_counts())plot_categories( titanic , cat = 'Embarked' , target = 'Survived' ) 输出： S 644C 168Q 77Name: Embarked, dtype: int64 可以看到S的数目是最多的，但是存活率是最低的 再看Sex与Survived的关系： 12print(titanic.Sex.value_counts())plot_categories( titanic , cat = 'Sex' , target = 'Survived' ) 输出： male 577female 314Name: Sex, dtype: int64可以看到女性人数少，但是有着绝对的更大的存活率。 对于Pclass与Survived的关系 12print(titanic.Pclass.value_counts())plot_categories( titanic , cat = 'Pclass' , target = 'Survived' ) 输出： 3 4911 2162 184Name: Pclass, dtype: int64等级1人最少，却有着最多的存活率，等级3人最多，却是最少的存活率 对于SibSp和Parch两个数据，可以进行求和，并分成0和不为0两类 1234titanic['Family_All'] = titanic['SibSp'] + titanic['Parch']titanic['Family_All'] = [0 if i == 0 else 1 for i in titanic.Family_All]print(titanic.Family_All.value_counts())plot_categories( titanic , cat = 'Family_All' , target = 'Survived' ) 输出： 0 5371 354Name: Family_All, dtype: int64可以看到为0的，存活率相较于不为0的，是要低很多的 大致的通过图表分析过后，对原始数据进行些处理。首先是将sex的male和female转为1和0 12my_sex = pd.DataFrame()my_sex['Sex'] = [1 if i == 'male' else 0 for i in full.Sex] Embarked数据存在极少量的缺失，这里打算用最多的‘S’来填补，再使用pd.get_dummies来将多个变量转为one_hot编码 1234my_embarked = pd.DataFrame()my_embarked['Embarked'] = full.Embarked.fillna('S')my_embarked = pd.get_dummies(my_embarked.Embarked, prefix = 'Embarked')my_embarked.head() 输出： 对于Pclass，没有缺失值，只需要转为one_hot即可 12my_pclass = pd.DataFrame()my_pclass = pd.get_dummies(full.Pclass, prefix='Pclass') 对于Fare，由于在测试集中有一个缺失值，所以可以采用平均数的方法来填补该缺失值，并且可以对Fare进行区间划分,并转为one_hot 12345my_fare = pd.DataFrame()my_fare['Fare'] = full.Fare.fillna(full.Fare.mean())my_fare['Fare'] = pd.qcut(my_fare['Fare'], 4)my_fare = pd.get_dummies(my_fare.Fare, prefix='Fare')my_fare.head() 对于Age，缺失值较多，可以根据已有数据的平均值和标准差随机生成填充数，并进行区间划分，转为one_hot 12345678910my_age = pd.DataFrame()my_age['Age'] = full.Ageage_avg = full.Age.mean()age_std = full.Age.std()age_null_count = full.Age.isnull().sum()age_null_random_list = np.random.randint(age_avg - age_std, age_avg + age_std, size = age_null_count)my_age['Age'][np.isnan(my_age['Age'])] = age_null_random_listmy_age['Age'] = pd.qcut(my_age['Age'], 4)my_age = pd.get_dummies(my_age.Age, prefix='Age')my_age.head() 根据Name中的内容可以生成title，并转为one_hot 12345678910111213141516171819202122232425title = pd.DataFrame()title['Title'] = full['Name'].map( lambda name: name.split( ',' )[1].split( '.' )[0].strip() )Title_Dictionary = &#123; "Capt": "Officer", "Col": "Officer", "Major": "Officer", "Jonkheer": "Royalty", "Don": "Royalty", "Sir" : "Royalty", "Dr": "Officer", "Rev": "Officer", "the Countess":"Royalty", "Dona": "Royalty", "Mme": "Mrs", "Mlle": "Miss", "Ms": "Mrs", "Mr" : "Mr", "Mrs" : "Mrs", "Miss" : "Miss", "Master" : "Master", "Lady" : "Royalty" &#125;title['Title'] = title.Title.map(Title_Dictionary)title = pd.get_dummies(title.Title)title.head() 输出： 对于Parch和SibSp，合并为Family_All 123my_family = pd.DataFrame()my_family['Family_All'] = full['Parch'] + full['SibSp']my_family['Family_All'] = [0 if i == 0 else 1 for i in my_family.Family_All] Cabin的缺失值过多，先暂时舍弃ticket也先舍弃 开始训练将刚才处理过的数据进行一个综合，并生成训练集和测试集 1234full_X = pd.concat( [my_family, title, my_age, my_embarked, my_fare, my_pclass, my_sex] , axis=1 )train_X = full_X[0:891]train_y = titanic.Survivedtest_X = full_X[891:] 选择模型，并进行5折交叉验证 123456from sklearn.model_selection import cross_val_scoremodel = GradientBoostingClassifier(learning_rate=0.01, max_depth=3, n_estimators=150)# model = SVC()# model = RandomForestClassifier(n_estimators=100)# model = DecisionTreeClassifier()cross_val_score(model, train_X, train_y, cv=5).mean() 输出： 0.8215596071618176 最后进行训练与预测 12345model.fit( train_X , train_y )test_Y = model.predict( test_X )passenger_id = full[891:].PassengerIdtest = pd.DataFrame( &#123; 'PassengerId': passenger_id , 'Survived': test_Y &#125; )test.to_csv( 'titanic_pred.csv' , index = False ) 最后上传到kaggle的成绩是0.78947，排在Top32%]]></content>
      <tags>
        <tag>机器学习</tag>
        <tag>数据分析</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《廖雪峰：SQL教程》笔记]]></title>
    <url>%2F2019%2F02%2F10%2F%E5%BB%96%E9%9B%AA%E5%B3%B0%20SQL%E6%95%99%E7%A8%8B%20%20%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[什么是SQL 简单地说，SQL就是访问和处理关系数据库的计算机标准语言具体来说，SQL是结构化查询语言Structured Query Language的缩写，用来访问和操作数据库系统。SQL语句既可以查询数据库中的数据，也可以添加、更新和删除数据库中的数据，还可以对数据库进行管理和维护操作。 数据库有哪三种模型 层次模型：就是以“上下级”的层次关系来组织数据的一种方式，层次模型的数据结构看起来就像一颗树网状模型：把每个数据节点和其他很多节点都连接起来，它的数据结构看起来就像很多城市之间的路网关系模型：把数据看作是一个二维表格，任何数据都可以通过行号+列号来唯一确定，它的数据模型看起来就是一个Excel表 主流的关系数据库有哪些 商用数据库，例如：Oracle，SQL Server，DB2等；开源数据库，例如：MySQL，PostgreSQL等;桌面数据库，以微软Access为代表，适合桌面应用程序使用；嵌入式数据库，以Sqlite为代表，适合手机应用和桌面程序。 SQL语言定义了哪几种操作数据库的能力 1.DDL：Data Definition Language，允许用户定义数据，也就是创建表、删除表、修改表结构这些操作。通常，DDL由数据库管理员执行。2.DML：Data Manipulation Language，DML为用户提供添加、删除、更新数据的能力，这些是应用程序对数据库的日常操作。3.DQL：Data Query Language，DQL允许用户查询数据，这也是通常最频繁的数据库日常操作。 关系模型 表的每一行称为记录（Record），记录是一个逻辑意义上的数据。 表的每一列称为字段（Column），同一个表的每一行记录都拥有相同的若干字段。字段定义了数据类型（整型、浮点型、字符串、日期等），以及是否允许为NULL NULL不表示0或者空字符串‘’，表示的是字段数据不存在。 通常情况下，字段应该避免允许为NULL。不允许为NULL可以简化查询条件，加快查询速度，也利于应用程序读取数据后无需判断是否为NULL。 和Excel表有所不同的是，关系数据库的表和表之间需要建立“一对多”，“多对一”和“一对一”的关系，这样才能够按照应用程序的逻辑来组织和存储数据。 主键也是一个字段，通过主键可以唯一确定一条记录。所有主键的选取是十分重要的，主键不要带有业务含义，如身份证号、手机号、邮箱地址，而应该使用BIGINT自增或者GUID类型。主键也不应该允许NULL。 关系数据库通过外键可以实现一对多、多对多和一对一的关系。 查询数据 要查询一张表的数据，使用：SELECT FROM &lt;表名&gt;，SELECT是关键字，表示要执行一次查询，表示所有列，FROM表示要从哪个表来查询 SELECT语句可以通过WHERE来设定查询条件，查询结果是满足查询条件的记录，即SELECT FROM &lt;表名&gt; WHERE &lt;条件表达式&gt;，如：SELECT FROM students WHERE score &gt;= 80，score是列名 条件表达式： &lt;条件1&gt; AND &lt;条件2&gt;：表示同时满足条件1条件2 &lt;条件1&gt; OR &lt;条件2&gt;：表示满足条件1或条件2 NOT &lt;条件&gt;：表示“不符合该条件”的记录 查询部分列：SELECT 列1, 列2, 列3 FROM &lt;表名&gt;，并且可以对这些列进行重命名：SELECT 列1 别名1, 列2 别名2, 列3 别名3 FROM &lt;表名&gt; 查询的结果默认是按主键排序的，而加上ORDER BY &lt;列名&gt;，表示根据该列按升序排列，使用ORDER BY &lt;列名&gt; DESC，表示按降序，对于相同的数据可以进一步排序：ORDER BY &lt;列名1&gt; DESC, &lt;列名2&gt; 使用LIMIT OFFSET 可以对结果集进行分页，每次查询返回结果集的一部分，LIMIT值是每页显示的个数，OFFSET值表示跳过前多少个；在MySQL中，LIMIT 15 OFFSET 30还可以简写成LIMIT 30, 15。 聚合查询： COUNT()：表示查询所有列的行数，如: SELECT COUNT(*) num FROM students; 会返回students表的总行数，且命名为num SUM(id) 计算某一列的合计值，该列必须为数值类型 AVG(id) 计算某一列的平均值，该列必须为数值类型 MAX(id) 计算某一列的最大值，如果是字符类型，会返回排序最后的字符 MIN(id) 计算某一列的最小值，如果是字符类型，会返回排序最前的字符 可以综合起来使用，如：SELECT class_id, gender, COUNT(*) num FROM students GROUP BY class_id, gender; 其中GROUP BY子句指定了按class_id和gender来分组，会返回如下： 多表查询 SELECT * FROM &lt;表1&gt;, &lt;表2&gt; 表示表1和表2的“乘积”，列数是两表列数之和，行数是两表行数之积 多表查询时，可以使用表名.列名来SELECT这个列 可以使用FROM &lt;表1&gt; 别名1, &lt;表2&gt; 别名2，来给表1表2起别名，使用别名不是必须的，但可以更好地简化查询语句。 多表查询的结果集可能非常巨大，要小心使用 连接查询 先确定主表，仍然使用FROM &lt;表1&gt;的语法； 再确定需要连接的表，使用INNER JOIN &lt;表2&gt;的语法； 然后确定连接条件，使用ON &lt;条件…&gt;，这里的条件是s.class_id = c.id，表示students表的class_id列与classes表的id列相同的行需要连接； 可选：加上WHERE子句、ORDER BY等子句。 RIGHT OUTER JOIN返回右表都存在的行，如果某一行仅在右表存在，那么结果集就会以NULL填充剩下的字段 LEFT OUTER JOIN则返回左表都存在的行，如果某一行仅在左表存在，那么结果集就会以NULL填充剩下的字段 FULL OUTER JOIN，它会把两张表的所有记录全部选择出来，并且，自动把对方不存在的列填充为NULL 修改数据 INSERT： 基本语法为：INSERT INTO &lt;表名&gt; (字段1, 字段2, …) VALUES (值1, 值2, …); 值的顺序必须和字段顺序一致 还可以一次性添加多条记录，只需要在VALUES子句中指定多个记录值，每个记录是由(…)包含的一组值： UPDATE： 基本语法为：UPDATE &lt;表名&gt; SET 字段1=值1, 字段2=值2, … WHERE …; 在字段1=值1, 字段2=值2… 写出要更新的值 在WHERE子句中写出需要更新的行的筛选条件 更新字段时可以使用表达式，如：score=score+10，表示符合where条件的分数都加10 注意：如果没有where条件，则所有的记录都将被update，因此，最好先用SELECT语句来测试WHERE条件是否筛选出了期望的记录集，然后再用UPDATE更新。 DELETE 基本语法为：DELETE FROM &lt;表名&gt; WHERE …; 和UPDATE相同，如果没有where条件，则会删除整个表的数据 MySQL 列出全部的数据库：SHOW DATABASES;其中，information_schema、mysql、performance_schema和sys是系统库，不要去改动它们 创建一个新数据库：CREATE DATABASE test; 删除一个数据库：DROP DATABASE test; 对一个数据库进行操作时，要首先将其切换为当前数据库：USE test; 列出当前数据库的所有表：SHOW TABLES; 要查看一个表的结构：DESC &lt;表名&gt;; 删除表：DROP TABLE students; 插入或替换：REPLACE INTO students (id, class_id, name, gender, score) VALUES (1, 1, ‘小明’, ‘F’, 99); 插入或更新：INSERT INTO students (id, class_id, name, gender, score) VALUES (1, 1, ‘小明’, ‘F’, 99) ON DUPLICATE KEY UPDATE name=’小明’, gender=’F’, score=99; 插入或忽略：INSERT IGNORE INTO students (id, class_id, name, gender, score) VALUES (1, 1, ‘小明’, ‘F’, 99); 对一个表进行快照，即复制一份当前表的数据到一个新表：CREATE TABLE students_of_class1 SELECT * FROM students WHERE class_id=1; 事物 数据库事物：把多条语句作为一个整体进行操作的功能，数据库事务可以确保该事务范围内的所有操作都可以全部成功或者全部失败 具有ACID四个特性： Atomic，原子性，将所有SQL作为原子工作单元执行，要么全部执行，要么全部不执行； Consistent，一致性，事务完成后，所有数据的状态都是一致的，即A账户只要减去了100，B账户则必定加上了100； Isolation，隔离性，如果有多个事务并发执行，每个事务作出的修改必须与其他事务隔离； Duration，持久性，即事务完成后，对数据库数据的修改被持久化存储。 对于单条SQL语句，数据库系统自动将其作为一个事务执行，这种事务被称为隐式事务。 要手动把多条SQL语句作为一个事务执行，使用BEGIN开启一个事务，使用COMMIT提交一个事务，这种事务被称为显式事务 COMMIT是指提交事务，即试图把事务内的所有SQL所做的修改永久保存 有些时候，我们希望主动让事务失败，这时，可以用ROLLBACK回滚事务，整个事务会失败 参考资料廖雪峰SQL教程]]></content>
      <tags>
        <tag>笔记</tag>
        <tag>SQL</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[《廖雪峰：Git教程》笔记]]></title>
    <url>%2F2019%2F02%2F03%2F%E5%BB%96%E9%9B%AA%E5%B3%B0%20Git%E6%95%99%E7%A8%8B%20%20%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[Git是什么? Git是目前世界上最先进的分布式版本控制系统 什么是版本控制系统? 对软件开发过程中各种程序代码、配置文件及说明文档等文件变更的管理 集中式和分布式版本控制系统有什么区别? CVS及SVN都是集中式的版本控制系统，而Git是分布式版本控制系统集中式版本控制系统，版本库是集中存放在中央服务器的，必须联网才能工作分布式版本控制系统根本没有“中央服务器”，每个人的电脑上都是一个完整的版本库,而集中式版本控制系统的中央服务器要是出了问题，所有人都没法干活了 什么是版本库 版本库又名仓库，英文名repository，可以简单理解成一个目录，这个目录里面的所有文件都可以被Git管理起来，每个文件的修改、删除，Git都能跟踪 工作区和暂存区 工作区就是电脑上可以看到的目录，git add就是把工作区中的修改提交到暂存区，然后git commit可以一次性把暂存区的所有修改提交到分支git diff 是工作区(work dict)和暂存区(stage)的比较git diff –cached 是暂存区(stage)和分支(master)的比较git diff HEAD – \是工作区和分支的比较 为什么Git比其他版本控制系统设计得优秀 因为Git跟踪并管理的是修改，而非文件。 基本操作 初始化一个Git仓库：使用git init命令。 添加文件到Git仓库，分两步： 使用命令git add “file”，该file需放到仓库文件夹下，可反复多次使用，添加多个文件 使用命令git commit -m “message”，一次性提交刚add的多个文件，message是对本次提交的一个说明 使用git status命令，可以掌握当前仓库的状态 如果git status显示某个file有修改，则使用git diff “file” 可以查看该file具体修改的地方 git log命令显示从最近到最远的提交日志,使用git log –pretty=oneline就会显示的比较简洁,前面会显示版本号! 使用git reset –hard HEAD^可以回退到上一个版本，HEAD^^则是上上个版本，HEAD~100则是上100个版本 使用git reset –hard “commit id”,可以再回退回去 如果不记得id了，可以使用git reflog，可以查看到变更的记录，当前的HEAD，以及之前版本的版本号) 使用git checkout – “file” 来丢弃工作区的修改 如果修改已经提交到了暂存区，但还没有commit，如果想丢弃修改，则先使用git reset HEAD “file”，将暂存区的修改撤销 如果已经commit，则需使用前面说过的版本回退 在工作区删除某个file之后，尚未add和commit： 如果后悔删除了，则可以使用git checkout – “file”来从版本库恢复该文件，git checkout其实是用版本库里的版本替换工作区的版本，无论工作区是修改还是删除，都可以“一键还原” 如果确定要删除，并且在版本库中也要删除掉的话，先使用git rm “file”,再git commit -m “message”,即可 关于分支： 查看当前的分支情况：git branch 创建分支：git branch “branch name” 切换到分支：git checkout “branch name” 上面两步可以合并成一个：git checkout -b “branch name” 切回主分支：git checkout master 合并某分支到当前分支上：git merge “branch name” 删除分支：git branch -d “branch name” 当无法自动合并时，需先解决冲突，解决冲突后再提交，完成合并。解决冲突就是把Git合并失败的文件手动编辑为我们希望的内容，再提交。 使用git log –graph命令可以看到分支合并图 合并时加上–no–ff，表示禁用FastForward，具体地是git merge –no-ff -m “message” “branch name”,这样会产生一个新的commit mster分支是十分稳定的，平时在dev分支上干活，每个人在自己的分支上干活，然后往dev上合并，最后版本发布时，再把dev合并到master上 修复bug时，我们会通过创建新的bug分支进行修复，然后合并，最后删除 当手头工作没有完成时，先把工作现场git stash一下，然后去修复bug，修复后，再git stash pop，回到工作现场，使用git stash list查看存储的工作现场 如果要丢弃一个没有被合并过的分支，可以通过git branch -D “branch name”强行删除 远程库： 使用git remote，查看远程库的信息，默认名称是origin 使用git remote -v，显示更为详细的信息 使用git push origin “local branch name”，来推送分支到远程库 如果推送失败，先用git pull抓取远程的新提交 在本地创建和远程分支对应的分支，使用git checkout -b branch-name origin/branch-name，本地和远程分支的名称最好一致 建立本地分支和远程分支的关联，使用git branch –set-upstream branch-name origin/branch-name 标签管理 使用git tag “tag name”, 用于给当前分支的commit创建一个标签 使用git tag，可以查看当前的全部标签 使用git show “tag name”, 查看该tag的具体信息 标签总是和某个commit挂钩。如果这个commit既出现在master分支，又出现在dev分支，那么在这两个分支上都可以看到这个标签。 使用git push origin “tagname”,推送一个标签到远程库 使用git push origin –tags，推送呢全部本地标签到远程库 使用git tag -d “tagname”，删除本地标签 使用git push origin :refs/tags/“tagname”，删除一个远程标签 使用git config –global alias.”别名” “原操作”，使用别名来代替原操作 小抄 参考资料廖雪峰-Git教程]]></content>
      <tags>
        <tag>笔记</tag>
        <tag>Git</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之pandas]]></title>
    <url>%2F2019%2F01%2F29%2FPython%E4%B9%8Bpandas%2F</url>
    <content type="text"><![CDATA[Series series是一个像数组一样的一维序列,并伴有一个数组表示label,叫做index,默认的index是0,1,2…,当然也可以在创建Series时设定好index 123456789101112131415obj = pd.Series([4, 7, 5, -3])obj# 0 4# 1 7# 2 5# 3 -3# dtype: int64obj2 = pd.Series([4, 7, 5, -3], index = ['a', 'b', 'c', 'd'])obj2# a 4# b 7# c 5# d -3# dtype: int64 使用numpy函数或类似的操作，会保留index-value的关系 1234567import numpy as npnp.exp(obj2)# a 54.598150# b 1096.633158# c 148.413159# d 0.049787# dtype: float64 另一种看待series的方法,它是一个长度固定,有顺序的dict,从index映射到value,因此也可以用现有的dict来创建series 123456789sdata = &#123;'Ohio': 35000, 'Texas': 71000, 'Oregon':16000, 'Utah': 5000&#125;obj3 = pd.Series(sdata)obj3# Ohio 35000# Texas 71000# Oregon 16000# Utah 5000# dtype: int64 pandas中的isnull和notnull函数可以用来检测缺失数据 12345678910111213141516171819202122states = ['California', 'Ohio', 'Oregon', 'Texas']obj4 = pd.Series(sdata, index=states)obj4# California NaN# Ohio 35000.0# Oregon 16000.0# Texas 71000.0# dtype: float64pd.isnull(obj4)# California True# Ohio False# Oregon False# Texas False# dtype: boolpd.notnull(obj4)# California False# Ohio True# Oregon True# Texas True# dtype: bool Series有个特色是自动按照index label来排序 123456789101112131415161718192021obj3# Ohio 35000# Texas 71000# Oregon 16000# Utah 5000# dtype: int64obj4# California NaN# Ohio 35000.0# Oregon 16000.0# Texas 71000.0# dtype: float64obj3 + obj4# California NaN# Ohio 70000.0# Oregon 32000.0# Texas 142000.0# Utah NaN# dtype: float64 series的index能被直接更改 1234567891011121314obj# 0 4# 1 7# 2 5# 3 -3# dtype: int64obj.index = ['Bob', 'Steve', 'Jeff', 'Ryan']obj# Bob 4# Steve 7# Jeff -5# Ryan 3# dtype: int64 DataFrame 用Excel来理解DataFrame是更为直观的 构建一个dataframe的方法，用一个dcit，dict里的值是list dataframe也会像series一样，自动给数据赋index 123456789101112131415data = &#123;'state': ['Ohio', 'Ohio', 'Ohio', 'Nevada', 'Nevada', 'Nevada'], 'year': [2000, 2001, 2002, 2001, 2002, 2003], 'pop': [1.5, 1.7, 3.6, 2.4, 2.9, 3.2]&#125; frame = pd.DataFrame(data)frame# state year pop# 0 Ohio 2000 1.5# 1 Ohio 2001 1.7# 2 Ohio 2002 3.6# 3 Nevada 2001 2.4# 4 Nevada 2002 2.9# 5 Nevada 2003 3.2 如果指定一列的话，则会自动按指定的列排序 12345678pd.DataFrame(data, columns=['year', 'state', 'pop'])# year state pop# 0 2000 Ohio 1.5# 1 2001 Ohio 1.7# 2 2002 Ohio 3.6# 3 2001 Nevada 2.4# 4 2002 Nevada 2.9# 5 2003 Nevada 3.2 从DataFrame里提取一列的话会返回series格式，可以以属性或是dict一样的形式来提取 1234567891011121314151617frame['state']# 0 Ohio# 1 Ohio# 2 Ohio# 3 Nevada# 4 Nevada# 5 Nevada# Name: state, dtype: objectframe.year# 0 2000# 1 2001# 2 2002# 3 2001# 4 2002# 5 2003# Name: year, dtype: int64 如果是提取一行的话,需要用到loc,loc中使用index 12345frame.loc[1]# state Ohio# year 2001# pop 1.7# Name: 1, dtype: object 如果把list或array赋给column的话，长度必须符合DataFrame的长度。如果把一二series赋给DataFrame，会按DataFrame的index来赋值，不够的地方用缺失数据来表示 12345678910111213141516171819202122232425262728293031frame['debt'] = 0frame# state year pop debt# 0 Ohio 2000 1.5 0# 1 Ohio 2001 1.7 0# 2 Ohio 2002 3.6 0# 3 Nevada 2001 2.4 0# 4 Nevada 2002 2.9 0# 5 Nevada 2003 3.2 0list = np.arange(6)frame['debt'] = listframe# state year pop debt# 0 Ohio 2000 1.5 0# 1 Ohio 2001 1.7 1# 2 Ohio 2002 3.6 2# 3 Nevada 2001 2.4 3# 4 Nevada 2002 2.9 4# 5 Nevada 2003 3.2 5val = pd.Series([-1.2, -1.3, -1.5], index = [1, 3, 5])frame['debt'] = valframe# state year pop debt# 0 Ohio 2000 1.5 NaN# 1 Ohio 2001 1.7 -1.2# 2 Ohio 2002 3.6 NaN# 3 Nevada 2001 2.4 -1.3# 4 Nevada 2002 2.9 NaN# 5 Nevada 2003 3.2 -1.5 任何对series的改变，会反映在DataFrame上。除非我们用copy方法来新建一个。 把上面这种嵌套dcit传给DataFrame，pandas会把外层dcit的key当做列，内层key当做行索引 123456789pop = &#123;'Nevada': &#123;2001: 2.4, 2002: 2.9&#125;, 'Ohio': &#123;2000: 1.5, 2001: 1.7, 2002: 3.6&#125;&#125;frame3 = pd.DataFrame(pop)frame3# Nevada Ohio# 2000 NaN 1.5# 2001 2.4 1.7# 2002 2.9 3.6 如果DataFrame的index和column有自己的name属性，也会被显示 12345678frame3.index.name = 'year'; frame3.columns.name = 'state'frame3# state Nevada Ohio# year # 2000 NaN 1.5# 2001 2.4 1.7# 2002 2.9 3.6 index object是不可被修改的 12345678obj = pd.Series(range(3), index=['a', 'b', 'c'])index = obj.indexindex# Index(['a', 'b', 'c'], dtype='object')index[1]# 'b'index[1] = 'd'# TypeError: Index does not support mutable operations 所以这个index不仅像数组,还有点像set,但是与set不同的是,index是可以重复的 1234obj = pd.Series(range(3), index=['a', 'c', 'c'])index = obj.indexindex# Index(['a', 'c', 'c'], dtype='object') 主要功能重新索引:reindex reindex:重新生成一个更改index的obj,如果没有对应的index,则会引入数据缺失 12345678910111213141516obj = pd.Series([4.5, 7.2, -5.3, 3.6], index=['d', 'b', 'a', 'c'])obj# d 4.5# b 7.2# a -5.3# c 3.6# dtype: float64obj2 = obj.reindex(['a', 'b', 'c', 'd', 'e'])obj2# a -5.3# b 7.2# c 3.6# d 4.5# e NaN# dtype: float64 对于DataFrame,reindex既可以更改row,也可以更改column 123456789101112131415161718192021222324frame = pd.DataFrame(np.arange(9).reshape(3, 3), index=['a', 'c', 'd'], columns=['Ohio', 'Texas', 'California'])frame# Ohio Texas California# a 0 1 2# c 3 4 5# d 6 7 8frame2 = frame.reindex(['a', 'b', 'c', 'd'])frame2# Ohio Texas California# a 0.0 1.0 2.0# b NaN NaN NaN# c 3.0 4.0 5.0# d 6.0 7.0 8.0states = ['Texas', 'Utah', 'California']frame3 =frame.reindex(columns=states)frame3# Texas Utah California# a 1 NaN 2# c 4 NaN 5# d 7 NaN 8 删除记录 对于series，drop回返回一个新的object，并删去指定的axis的值 1234567891011121314151617181920obj = pd.Series(np.arange(5.), index=['a', 'b', 'c', 'd', 'e'])obj# a 0.0# b 1.0# c 2.0# d 3.0# e 4.0# dtype: float64new_obj = obj.drop('c')new_obj# a 0.0# b 1.0# d 3.0# e 4.0# dtype: float64obj.drop(['b', 'c'])# a 0.0# d 3.0# e 4.0# dtype: float64 对于DataFrame，index能按行或列的axis来删除 删除行的,直接drop行的labels即可 删除列的,需指定axis = 1 12345678910111213141516171819202122data = pd.DataFrame(np.arange(16).reshape(4, 4), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])data# one two three four# Ohio 0 1 2 3# Colorado 4 5 6 7# Utah 8 9 10 11# New York 12 13 14 15data.drop(['Colorado', 'Ohio'])# one two three four# Utah 8 9 10 11# New York 12 13 14 15data.drop('two', axis=1)# one three four# Ohio 0 2 3# Colorado 4 6 7# Utah 8 10 11# New York 12 14 15 drop也可以不返回一个新的object,而是直接更改series or dataframe,设定inplace为True即可 1234567obj.drop('c', inplace=True)obj# a 0.0# b 1.0# d 3.0# e 4.0# dtype: float64 索引，选择，过滤 Series,可以用整数或者label来选中行,用label来切片的时候,和python的切片不一样的在于,会包括尾节点： 12345678910111213141516obj = pd.Series(np.arange(4.), index=['a', 'b', 'c', 'd'])obj# a 0.0# b 1.0# c 2.0# d 3.0# dtype: float64obj['b':'c']# b 1.0# c 2.0# dtype: float64obj[1:2]# b 1.0# dtype: float64 DataFrame,可以通过一个值或序列,选中一个以上的列,行选择的语法格式为data[:2],也有一些特别的用法,比如传入布尔数组 1234567891011121314151617181920212223242526data = pd.DataFrame(np.arange(16).reshape((4, 4)), index=['Ohio', 'Colorado', 'Utah', 'New York'], columns=['one', 'two', 'three', 'four'])data# one two three four# Ohio 0 1 2 3# Colorado 4 5 6 7# Utah 8 9 10 11# New York 12 13 14 15data['one']# Ohio 0# Colorado 4# Utah 8# New York 12# Name: one, dtype: int64data[:2]# one two three four# Ohio 0 1 2 3# Colorado 4 5 6 7data[data['three']&gt;6]# one two three four# Utah 8 9 10 11# New York 12 13 14 15 还有一种方法是boolean dataframe 1234567891011121314data &lt; 5# one two three four# Ohio True True True True# Colorado True False False False# Utah False False False False# New York False False False Falsedata[data&lt;5] = 0data# one two three four# Ohio 0 0 0 0# Colorado 0 5 6 7# Utah 8 9 10 11# New York 12 13 14 15 loc和iloc loc索引是通过labels iloc索引是通过整数 123456789101112131415161718192021222324252627282930data# one two three four# Ohio 0 1 2 3# Colorado 4 5 6 7# Utah 8 9 10 11# New York 12 13 14 15data.loc['Colorado']# one 0# two 5# three 6# four 7# Name: Colorado, dtype: int64data.iolc[2]# one 0# two 5# three 6# four 7# Name: Colorado, dtype: int64data.loc['Colorado', ['two', 'three']]# two 5# three 6# Name: Colorado, dtype: int64data.iloc[1, [1,2]]# two 5# three 6# Name: Colorado, dtype: int64 1* loc和iloc也可以进行切片 1234567891011data.loc[:'Utah', 'two']# Ohio 0# Colorado 5# Utah 9# Name: two, dtype: int64data.iloc[:3, 1]# Ohio 0# Colorado 5# Utah 9# Name: two, dtype: int64 算数与数据对齐 如果两个object相加,但他们各自的index并不相同,最后结果得到的index是这两个index的合集 在DataFrame中,数据对齐同时发生在行和列上 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748s1 = pd.Series([7.3, -2.5, 3.4, 1.5], index=['a', 'c', 'd', 'e'])s2 = pd.Series([2.1, 3.6, -1.5, 4, 3.1], index=['a', 'c', 'e', 'f', 'g'])s1# a 7.3# c -2.5# d 3.4# e 1.5# dtype: float64s2# a 2.1# c 3.6# e -1.5# f 4.0# g 3.1# dtype: float64s1+s2# a 9.4# c 1.1# d NaN# e 0.0# f NaN# g NaN# dtype: float64df1 = pd.DataFrame(np.arange(9.).reshape((3, 3)), columns=['b', 'c', 'd'], index=['Ohio', 'Texas', 'Colorado'])df2 = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=['b', 'd', 'e'], index=['Utah', 'Ohio', 'Texas', 'Oregon'])df1# b c d# Ohio 0.0 1.0 2.0# Texas 3.0 4.0 5.0# Colorado 6.0 7.0 8.0df2# b d e# Utah 0.0 1.0 2.0# Ohio 3.0 4.0 5.0# Texas 6.0 7.0 8.0# Oregon 9.0 10.0 11.0df1+df2# b c d e# Colorado NaN NaN NaN NaN# Ohio 3.0 NaN 6.0 NaN# Oregon NaN NaN NaN NaN# Texas 9.0 NaN 12.0NaN# Utah NaN NaN NaN NaN 使用fill_value可以对缺失值进行填充,fill_value为0时,填充的值是原来的值 1234567df1.add(df2, fill_value=0)# b c d e# Colorado 6.0 7.0 8.0 NaN# Ohio 3.0 1.0 6.0 5.0# Oregon 9.0 NaN 10.0 11.0# Texas 9.0 4.0 12.0 8.0# Utah 0.0 NaN 1.0 2.0 DataFrame和Series之间的操作 series的index和dataframe的列匹配,向下按行进行广播 123456789101112131415161718192021222324frame = pd.DataFrame(np.arange(12.).reshape((4, 3)), columns=['b','d','e'], index=['Utah', 'Ohio', 'Texas', 'Oregon'])series = frame.iloc[0]frame# b d e# Utah 0.0 1.0 2.0# Ohio 3.0 4.0 5.0# Texas 6.0 7.0 8.0# Oregon 9.0 10.011.0series# b 0.0# d 1.0# e 2.0# Name: Utah, dtype: float64frame - series# b d e# Utah 0.0 0.0 0.0# Ohio 3.0 3.0 3.0# Texas 6.0 6.0 6.0# Oregon 9.0 9.0 9.0 如果一个index既不在DataFrame的column中,也不再series里的index中,那么结果也是合集 1234567series2 = pd.Series(range(3), index=['b', 'e', 'f'])frame + series2# b d e f# Utah 0.0 NaN 3.0 NaN# Ohio 3.0 NaN 6.0 NaN# Texas 6.0 NaN 9.0 NaN# Oregon 9.0 NaN 12.0 NaN 如果想要广播列，去匹配行，必须要用到算数方法 12345678910111213141516171819frame# b d e# Utah 0.0 1.0 2.0# Ohio 3.0 4.0 5.0# Texas 6.0 7.0 8.0# Oregon 9.0 10.011.0series3 = frame['d']series3# Utah 1.0# Ohio 4.0# Texas 7.0# Oregon 10.0# Name: d, dtype: float64frame.sub(series3, axis='index')# b d e# Utah -1.0 0.0 1.0# Ohio -1.0 0.0 1.0# Texas -1.0 0.0 1.0# Oregon-1.0 0.0 1.0 函数应用和映射 把一个用在一维数组上的函数，应用在一行或一列上,要用到DataFrame中的apply函数 默认是应用在每一列,也可以设置axis来应用在每一行 1234567891011121314151617181920frame# b d e# Utah 0.0 1.0 2.0# Ohio 3.0 4.0 5.0# Texas 6.0 7.0 8.0# Oregon 9.0 10.011.0f = lambda x: x.max() - x.min()frame.apply(f)# b 9.0# d 9.0# e 9.0# dtype: float64frame.apply(f, axis='columns')# Utah 2.0# Ohio 2.0# Texas 2.0# Oregon 2.0# dtype: float64 对于应用在每个元素的函数,需要applymap函数 比如下例中的将每个元素转成保留小数点两位的浮点数 1234567format = lambda x: '%.2f' % xframe.applymap(format)# b d e# Utah 0.00 1.00 2.00# Ohio 3.00 4.00 5.00# Texas 6.00 7.00 8.00# Oregon 9.00 10.00 11.00 排序和排名 使用sort_index函数来排序index,默认排的是index,也可以设置axis=1来排序column,默认是升序,也可以设置ascending为False为降序 12345678910111213141516171819frame = pd.DataFrame(np.arange(8).reshape((2, 4)), index=['three', 'one'], columns=['d', 'a', 'b', 'c'])frame# d a b c# three 0 1 2 3# one 4 5 6 7frame.sort_index()# d a b c# one 4 5 6 7# three 0 1 2 3frame.sort_index(axis=1)# a b c d# three 1 2 3 0# one 5 6 7 4frame.sort_index(axis=1, ascending=False)# d c b a# three 0 3 2 1# one 4 7 6 5 用sort_values方法来排序值,缺失值会排在最后 DataFrame通过by选择一列或者多列,多列的话是以list的形式传入 12345678910111213141516171819202122232425262728293031obj = pd.Series([4, np.nan, 7, np.nan, -3, 2])obj.sort_values()# 4 -3.0# 5 2.0# 0 4.0# 2 7.0# 1 NaN# 3 NaN# dtype: float64frame = pd.DataFrame(&#123;'b': [4, 7, -3, 2], 'a': [0, 1, 0, 1]&#125;)frame# b a# 0 4 0# 1 7 1# 2 -3 0# 3 2 1frame.sort_values(by='b')# b a# 2 -3 0# 3 2 1# 0 4 0# 1 7 1frame.sort_values(by=['a', 'b'])# b a# 2 -3 0# 0 4 0# 3 2 1# 1 7 1 rank函数表示在这个数在原来的Series中排第几名，有相同的数，取其排名平均（默认）作为值 12345678910111213141516171819202122232425262728obj = pd.Series([7, -5, 7, 4, 2, 0, 4])obj# 0 7# 1 -5# 2 7# 3 4# 4 2# 5 0# 6 4# dtype: int64obj.sort_values()# 1 -5# 5 0# 4 2# 3 4# 6 4# 0 7# 2 7# dtype: int64obj.rank()# 0 6.5# 1 1.0# 2 6.5# 3 4.5# 4 3.0# 5 2.0# 6 4.5# dtype: float64 dataframe 可以根据行或列来计算rank 123456789101112131415frame = pd.DataFrame(&#123;'b': [4.3, 7, -3, 2], 'a': [0, 1, 0, 1], 'c': [-2, 5, 8, -2.5]&#125;)frame# a b c# 0 0 4.3 -2.0# 1 1 7.0 5.0# 2 0 -3.0 8.0# 3 1 2.0 -2.5frame.rank(axis='columns')# a b c# 0 2.0 3.0 1.0# 1 1.0 3.0 2.0# 2 2.0 1.0 3.0# 3 2.0 3.0 1.0 重复 对于有重复的index或者column,可以使用is_unique来判断 123obj = pd.Series(np.arange(5), index=['a', 'a', 'b', 'b', 'c'])obj.index.is_unique# False 数据选择,对于有重复的label,会返回一个Series,否则返回值 123456obj['a']# a 0# a 1# dtype: int64obj['c']# 4 同样应用于DataFrame,如果有重复,返回DataFrame,否则返回Series 1234567891011121314151617df = pd.DataFrame(np.random.randn(5, 3), index=['a', 'a', 'b', 'b', 'c'])df# 0 1 2# a 0.413265 -1.520993 0.211549# a 0.192379 0.119111 -0.637629# b -0.400623 0.455354 0.059163# b -0.278948 1.009609 -0.859333# c -0.221088 -1.393599 -0.311840df.loc['a']# 0 1 2# a 0.413265 -1.520993 0.211549# a 0.192379 0.119111 -0.637629df.loc['c']# 0 -0.221088# 1 -1.393599# 2 -0.311840# Name: c, dtype: float64 值计数 可以从一维的Series中提取信息,使用unique函数,返回除去重复的还有哪些值 1234567891011121314obj = pd.Series(['c', 'a', 'd', 'a', 'a', 'b', 'b', 'c', 'c'])obj# 0 c# 1 a# 2 d# 3 a# 4 a# 5 b# 6 b# 7 c# 8 c# dtype: objectobj.unique()# array(['c', 'a', 'd', 'b'], dtype=object) value_counts能计算series中值出现的频率 123456obj.value_counts()# c 3# a 3# b 2# d 1# dtype: int64 isin 能实现一个向量化的集合成员关系检查，能用于过滤数据集，检查一个子集，是否在series的values中，或在dataframe的column中 1234567891011obj.isin(['b', 'c'])# 0 True# 1 False# 2 False# 3 False# 4 False# 5 True# 6 True# 7 True# 8 True# dtype: bool 汇总和描述性统计 从series中提取单个值（比如sum或mean） 计算的时候，NA（即缺失值）会被除外，除非整个切片全是NA。我们可以用skipna来跳过计算NA 123456789101112131415161718192021222324252627282930313233343536df = pd.DataFrame([[1.4, np.nan], [7.1, -4.5], [np.nan, np.nan], [0.75, -1.3]], index=['a', 'b', 'c', 'd'], columns=['one', 'two'])df# one two# a 1.40 NaN# b 7.10 -4.5# c NaN NaN# d 0.75 -1.3df.sum()# one 9.25# two -5.80# dtype: float64df.sum(axis='columns')# a 1.40# b 2.60# c 0.00# d -0.55# dtype: float64df.mean(axis='columns)# a 1.400# b 1.300# c NaN# d -0.275# dtype: float64df.mean(axis = 'columns', skipna = False)# a NaN# b 1.300# c NaN# d -0.275# dtype: float64 还有idxmin和idxmax能返回最大最小的index,cumsum能进行累加 1234567891011df.idxmax()# one b# two d# dtype: objectdf.cumsum()# one two# a 1.40 NaN# b 8.50 -4.5# c NaN NaN# d 9.25 -5.8 describe能一下子产生多维汇总数据 对于非数值性的数据，describe能产生另一种汇总统计 12345678910111213141516171819202122232425262728293031323334353637df.describe()# one two# count 3.000000 2.000000# mean 3.083333 -2.900000# std 3.493685 2.262742# min 0.750000 -4.500000# 25% 1.075000 -3.700000# 50% 1.400000 -2.900000# 75% 4.250000 -2.100000# max 7.100000 -1.300000obj = pd.Series(['a', 'a', 'b', 'c'] * 4)obj# 0 a# 1 a# 2 b# 3 c# 4 a# 5 a# 6 b# 7 c# 8 a# 9 a# 10 b# 11 c# 12 a# 13 a# 14 b# 15 c# dtype: objectobj.describe()# count 16# unique 3# top a# freq 8# dtype: object 本文主要参考于:https://nbviewer.jupyter.org/github/LearnXu/pydata-notebook/tree/master/Chapter-05/]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[LeetCode之哈希表]]></title>
    <url>%2F2019%2F01%2F27%2FLeetCode-Hash%2F</url>
    <content type="text"><![CDATA[705. 设计哈希集合难度:容易要求 不使用任何内建的哈希表库设计一个哈希集合具体地说，你的设计应该包含以下的功能add(value)：向哈希集合中插入一个值。contains(value) ：返回哈希集合中是否存在这个值。remove(value)：将给定值从哈希集合中删除。如果哈希集合中没有这个值，什么也不做。 示例 MyHashSet hashSet = new MyHashSet();hashSet.add(1);hashSet.add(2);hashSet.contains(1); // 返回 truehashSet.contains(3); // 返回 false (未找到)hashSet.add(2);hashSet.contains(2); // 返回 truehashSet.remove(2);hashSet.contains(2); // 返回 false (已经被删除) python代码 思路：利用字典，add时将key作为键，其对应的值设为1；remove时，将key对应的值设为0；检查是否存在时，如果字典中有key，返回其值是否等于1，若无则返回False 123456789101112131415161718192021222324252627282930313233343536373839class MyHashSet(object): def __init__(self): """ Initialize your data structure here. """ self.data = &#123;&#125; def add(self, key): """ :type key: int :rtype: void """ self.data[key] = 1 def remove(self, key): """ :type key: int :rtype: void """ self.data[key] = 0 def contains(self, key): """ Returns true if this set contains the specified element :type key: int :rtype: bool """ if key in self.data: return self.data[key] == 1 return False# Your MyHashSet object will be instantiated and called as such:# obj = MyHashSet()# obj.add(key)# obj.remove(key)# param_3 = obj.contains(key C++代码1234567891011121314151617181920212223242526272829class MyHashSet &#123;public: /** Initialize your data structure here. */ vector&lt;bool&gt; data; MyHashSet() &#123; data = vector&lt;bool&gt;(1000001,false); &#125; void add(int key) &#123; data[key]=true; &#125; void remove(int key) &#123; data[key]=false; &#125; /** Returns true if this set contains the specified element */ bool contains(int key) &#123; return data[key]; &#125;&#125;;/** * Your MyHashSet object will be instantiated and called as such: * MyHashSet obj = new MyHashSet(); * obj.add(key); * obj.remove(key); * bool param_3 = obj.contains(key); */ 706. 设计哈希映射难度:容易要求 不使用任何内建的哈希表库设计一个哈希映射具体地说，你的设计应该包含以下的功能:put(key, value)：向哈希映射中插入(键,值)的数值对。如果键对应的值已经存在，更新这个值。get(key)：返回给定的键所对应的值，如果映射中不包含这个键，返回-1。remove(key)：如果映射中存在这个键，删除这个数值对。 示例 MyHashMap hashMap = new MyHashMap();hashMap.put(1, 1);hashMap.put(2, 2);hashMap.get(1); // 返回 1hashMap.get(3); // 返回 -1 (未找到)hashMap.put(2, 1); // 更新已有的值hashMap.get(2); // 返回 1hashMap.remove(2); // 删除键为2的数据hashMap.get(2); // 返回 -1 (未找到) Python代码12345678910111213141516171819202122232425262728293031323334353637383940414243444546class MyHashMap(object): def __init__(self): """ Initialize your data structure here. """ self.data = &#123;&#125; def put(self, key, value): """ value will always be non-negative. :type key: int :type value: int :rtype: void """ self.data[key] = value def get(self, key): """ Returns the value to which the specified key is mapped, or -1 if this map contains no mapping for the key :type key: int :rtype: int """ if key in self.data: return self.data[key] else: return -1 def remove(self, key): """ Removes the mapping of the specified value key if this map contains a mapping for the key :type key: int :rtype: void """ if key in self.data: del self.data[key] else: return# Your MyHashMap object will be instantiated and called as such:# obj = MyHashMap()# obj.put(key,value)# param_2 = obj.get(key)# obj.remove(key) 217. 存在重复元素难度:容易要求 给定一个整数数组，判断是否存在重复元素。如果任何值在数组中出现至少两次，函数返回 true。如果数组中每个元素都不相同，则返回 false。 示例 输入: [1,2,3,1]输出: true 输入: [1,2,3,4]输出: false Python代码 我的 123456789101112class Solution(object): def containsDuplicate(self, nums): """ :type nums: List[int] :rtype: bool """ len_1 = len(nums) ans = set() for i in nums: ans.add(i) len_2 = len(ans) return (len_1 != len_2) 大佬的 1234567class Solution(object): def containsDuplicate(self, nums): """ :type nums: List[int] :rtype: bool """ return len(set(nums)) != len(nums) 136. 只出现一次的数字难度:容易要求 给定一个非空整数数组，除了某个元素只出现一次以外，其余每个元素均出现两次。找出那个只出现了一次的元素。说明：你的算法应该具有线性时间复杂度。 你可以不使用额外空间来实现吗？ 示例 输入: [2,2,1]输出: 1 输入: [4,1,2,1,2]输出: 4 Python代码 我的：采用了字典统计每个数出现的次数，最后返回次数为1的 123456789101112131415class Solution(object): def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ count = &#123;&#125; for i in nums: if i not in count: count[i] = 1 else: count[i] += 1 for j in count.keys(): if count[j] == 1: return j 大佬的：利用了异或的性质，出现了两次的异或后为0，出现了一次的与0异或得本身，最后返回即可 12345678910class Solution(object): def singleNumber(self, nums): """ :type nums: List[int] :rtype: int """ a=nums[0] for i in range(1,len(nums)): a=a^nums[i] return a 349. 两个数组的交集难度:容易要求 给定两个数组，编写一个函数来计算它们的交集。 示例 输入: nums1 = [1,2,2,1], nums2 = [2,2]输出: [2] 输入: nums1 = [4,9,5], nums2 = [9,4,9,8,4]输出: [9,4] Python 代码1234567891011121314151617class Solution(object): def intersection(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: List[int] """ nums1 = set(nums1) nums2 = set(nums2) ans = [] for i in nums2: if i in nums1: ans.append(i) return ans 202. 快乐数难度:容易要求 编写一个算法来判断一个数是不是“快乐数”。一个“快乐数”定义为：对于一个正整数，每一次将该数替换为它每个位置上的数字的平方和，然后重复这个过程直到这个数变为 1，也可能是无限循环但始终变不到 1。如果可以变为 1，那么这个数就是快乐数。 示例 输入: 19输出: true解释:12 + 92 = 8282 + 22 = 6862 + 82 = 10012 + 02 + 02 = 1 Python代码思路: 平方和会进入到循环中： 4 → 16 → 37 → 58 → 89 → 145 → 42 → 20 → 4，所以当平方和为这些循环中的值时，就不是快乐数， 12345678910111213141516class Solution(object): def isHappy(self, n): """ :type n: int :rtype: bool """ list1 = [4,16, 37, 58, 89, 145,42, 20] while(n != 1): temp = 0 while(n&gt;0): temp += (n%10) * (n%10) n = n/10 if temp in list1: return False n = temp return True 1. 两数之和难度:容易要求 给定一个整数数组 nums 和一个目标值 target，请你在该数组中找出和为目标值的那两个整数，并返回他们的数组下标。你可以假设每种输入只会对应一个答案。但是，你不能重复利用这个数组中同样的元素。 示例 给定 nums = [2, 7, 11, 15], target = 9因为 nums[0] + nums[1] = 2 + 7 = 9所以返回 [0, 1] Python代码 思路：建立一个字典，键为nums[i],值为下标i,遍历数组时，检查差值是否在字典中，若在，返回两个数的下标即可 12345678910111213class Solution(object): def twoSum(self, nums, target): """ :type nums: List[int] :type target: int :rtype: List[int] """ dic = &#123;&#125; for i in range(len(nums)): complement = target - nums[i] if complement in dic: return [dic[complement], i] dic[nums[i]] = i 205. 同构字符串要求 给定两个字符串 s 和 t，判断它们是否是同构的。如果 s 中的字符可以被替换得到 t ，那么这两个字符串是同构的。所有出现的字符都必须用另一个字符替换，同时保留字符的顺序。两个字符不能映射到同一个字符上，但字符可以映射自己本身。 示例 输入: s = “egg”, t = “add”输出: true 输入: s = “foo”, t = “bar”输出: false Python代码1234567891011121314151617181920212223class Solution(object): def isIsomorphic(self, s, t): """ :type s: str :type t: str :rtype: bool """ return self.iso(s,t) and self.iso(t,s) def iso(self,s, t): """ :type s: str :type t: str :rtype: bool """ mapx = &#123;&#125; for i in range(len(s)): if s[i] not in mapx: mapx[s[i]] = t[i] elif s[i] in mapx: if t[i] != mapx[s[i]]: return False return True 599. 两个列表的最小索引总和难度:容易要求 假设Andy和Doris想在晚餐时选择一家餐厅，并且他们都有一个表示最喜爱餐厅的列表，每个餐厅的名字用字符串表示。你需要帮助他们用最少的索引和找出他们共同喜爱的餐厅。 如果答案不止一个，则输出所有答案并且不考虑顺序。 你可以假设总是存在一个答案。 示例 输入：[“Shogun”, “Tapioca Express”, “Burger King”, “KFC”][“KFC”, “Shogun”, “Burger King”]输出: [“Shogun”]解释: 他们共同喜爱且具有最小索引和的餐厅是“Shogun”，它有最小的索引和1(0+1)。 python代码12345678910111213141516class Solution(object): def findRestaurant(self, list1, list2): """ :type list1: List[str] :type list2: List[str] :rtype: List[str] """ dic = &#123;&#125; for i in range(len(list1)): if list1[i] in list2: sum_index = i + list2.index(list1[i]) if sum_index not in dic: dic[sum_index] = [] dic[sum_index].append(list1[i]) min_key = min(dic.keys()) return dic[min_key] 387. 字符串中的第一个唯一字符难度:容易要求 给定一个字符串，找到它的第一个不重复的字符，并返回它的索引。如果不存在，则返回 -1。 示例 s = “leetcode”返回 0. s = “loveleetcode”,返回 2. python代码 维护一个hash表，键为字符，值为出现个数，返回第一个值为1的键的下标 123456789101112131415class Solution: def firstUniqChar(self, s): """ :type s: str :rtype: int """ str_hash = &#123;&#125; for i in range(len(s)): if s[i] not in str_hash: str_hash[s[i]] = 0 str_hash[s[i]] += 1 for i in str_hash.keys(): if str_hash[i] == 1: return s.index(i) return -1 350. 两个数组的交集 II难度:容易要求 给定两个数组，编写一个函数来计算它们的交集。 示例 输入: nums1 = [1,2,2,1], nums2 = [2,2]输出: [2,2] 输入: nums1 = [4,9,5], nums2 = [9,4,9,8,4]输出: [4,9] Python代码123456789101112131415161718class Solution: def intersect(self, nums1, nums2): """ :type nums1: List[int] :type nums2: List[int] :rtype: List[int] """ dic = &#123;&#125; ans = [] for i in nums1: if i not in dic: dic[i] = 0 dic[i] += 1 for i in nums2: if i in dic and dic[i] != 0: ans.append(i) dic[i] -= 1 return ans 219. 存在重复元素 II难度:容易要求 给定一个整数数组和一个整数 k，判断数组中是否存在两个不同的索引 i 和 j，使得 nums [i] = nums [j]，并且 i 和 j 的差的绝对值最大为 k。 示例 输入: nums = [1,2,3,1], k = 3输出: true 输入: nums = [1,0,1,1], k = 1输出: true 输入: nums = [1,2,3,1,2,3], k = 2输出: false Python代码 题目意思描述的不是很清楚，其实想表达的意思是在k个距离之内，nums中是否存在重复，可以用一个字典，键为nums中的数值，值为数值对应的下标，如果出现重复则计算下标差值是否在k之内，若在，则返回True，否则更新该数值所对应的字典中的值 1234567891011121314151617class Solution: def containsNearbyDuplicate(self, nums, k): """ :type nums: List[int] :type k: int :rtype: bool """ distance = &#123;&#125; for i in range(len(nums)): if nums[i] not in distance: distance[nums[i]] = i else: if i - distance[nums[i]] &lt;= k: return True else: distance[nums[i]] = i return False 49. 字母异位词分组难度:中等要求给定一个字符串数组，将字母异位词组合在一起。字母异位词指字母相同，但排列不同的字符串。 示例输入: [“eat”, “tea”, “tan”, “ate”, “nat”, “bat”],输出:[ [“ate”,”eat”,”tea”], [“nat”,”tan”], [“bat”]] python代码 思路：建立一个键值对，其中键的设计很重要，键可以设计成每一个字符串排序后的结果，这样每读取一个字符串，先进行一次重排，如果重排后的新串在字典中，则将原本的字符串插入到该键对应的值中，如果不在字典中，则建立一个新的键值对 12345678910111213141516171819202122232425class Solution: def rebuild(self, str): """ 重排字符串 """ ans = [] for i in str: ans.append(i) ans.sort() return ''.join(ans) def groupAnagrams(self, strs): """ :type strs: List[str] :rtype: List[List[str]] """ dic = &#123;&#125; for str in strs: if str in dic: dic[str].append(str) continue str_ = self.rebuild(str) if str_ not in dic: dic[str_] = [] dic[str_].append(str) return list(dic.values()) 36. 有效的数独难度:中等要求 判断一个 9x9 的数独是否有效。只需要根据以下规则，验证已经填入的数字是否有效即可。数字 1-9 在每一行只能出现一次。数字 1-9 在每一列只能出现一次。数字 1-9 在每一个以粗实线分隔的 3x3 宫内只能出现一次。 示例 输入:[[“5”,”3”,”.”,”.”,”7”,”.”,”.”,”.”,”.”],[“6”,”.”,”.”,”1”,”9”,”5”,”.”,”.”,”.”],[“.”,”9”,”8”,”.”,”.”,”.”,”.”,”6”,”.”],[“8”,”.”,”.”,”.”,”6”,”.”,”.”,”.”,”3”],[“4”,”.”,”.”,”8”,”.”,”3”,”.”,”.”,”1”],[“7”,”.”,”.”,”.”,”2”,”.”,”.”,”.”,”6”],[“.”,”6”,”.”,”.”,”.”,”.”,”2”,”8”,”.”],[“.”,”.”,”.”,”4”,”1”,”9”,”.”,”.”,”5”],[“.”,”.”,”.”,”.”,”8”,”.”,”.”,”7”,”9”]]输出: true 输入:[[“8”,”3”,”.”,”.”,”7”,”.”,”.”,”.”,”.”],[“6”,”.”,”.”,”1”,”9”,”5”,”.”,”.”,”.”],[“.”,”9”,”8”,”.”,”.”,”.”,”.”,”6”,”.”],[“8”,”.”,”.”,”.”,”6”,”.”,”.”,”.”,”3”],[“4”,”.”,”.”,”8”,”.”,”3”,”.”,”.”,”1”],[“7”,”.”,”.”,”.”,”2”,”.”,”.”,”.”,”6”],[“.”,”6”,”.”,”.”,”.”,”.”,”2”,”8”,”.”],[“.”,”.”,”.”,”4”,”1”,”9”,”.”,”.”,”5”],[“.”,”.”,”.”,”.”,”8”,”.”,”.”,”7”,”9”]]输出: false解释: 除了第一行的第一个数字从 5 改为 8 以外，空格内其他数字均与 示例1 相同。但由于位于左上角的 3x3 宫内有两个 8 存在, 因此这个数独是无效的。 python代码 思路：建立三个函数，分别是判断题目中的三个要求 123456789101112131415161718192021222324252627282930313233343536class Solution: def isValidRow(self, board): for i in range(9): list_ = [] for j in range(9): if board[i][j] != '.' and board[i][j] in list_: return False list_.append(board[i][j]) return True def isValidColumn(self, board): for j in range(9): list_ = [] for i in range(9): if board[i][j] != '.' and board[i][j] in list_: return False list_.append(board[i][j]) return True def isValidSquare(self, board): for i in range(0, 9, 3): for j in range(0, 9, 3): list_ = [] for k in range(i, i+3): for l in range(j, j+3): if board[k][l] != '.' and board[k][l] in list_: return False list_.append(board[k][l]) return True def isValidSudoku(self, board): """ :type board: List[List[str]] :rtype: bool """ return self.isValidRow(board) and self.isValidColumn(board) and self.isValidSquare(board) 652. 寻找重复的子树难度:中等要求 给定一棵二叉树，返回所有重复的子树。对于同一类的重复子树，你只需要返回其中任意一棵的根结点即可。两棵树重复是指它们具有相同的结构以及相同的结点值。 Python 代码 思路：对每一个子树采用先序遍历的方法，将遍历的结果保存为字符串作为键，值设为出现的次数，若值为1，则将子串的root添加到ans中，最后返回ans 123456789101112131415161718192021class Solution: def helper(self, root, dic, ans): if root is None: return '#' dic_key = str(root.val)+','+self.helper(root.left, dic, ans)+','+self.helper(root.right, dic, ans) if dic_key not in dic: dic[dic_key] = 0 elif dic[dic_key] == 1: ans.append(root) dic[dic_key] += 1 return dic_key def findDuplicateSubtrees(self, root): """ :type root: TreeNode :rtype: List[TreeNode] """ dic = &#123;&#125; ans = [] self.helper(root, dic, ans) return ans 771.宝石与石头难度:容易要求 给定字符串J 代表石头中宝石的类型，和字符串 S代表你拥有的石头。 S 中每个字符代表了一种你拥有的石头的类型，你想知道你拥有的石头中有多少是宝石。J 中的字母不重复，J 和 S中的所有字符都是字母。字母区分大小写，因此”a”和”A”是不同类型的石头。 示例 输入: J = “aA”, S = “aAAbbbb”输出: 3 输入: J = “z”, S = “ZZ”输出: 0 Python代码1234567891011121314class Solution: def numJewelsInStones(self, J, S): """ :type J: str :type S: str :rtype: int """ dic = &#123;&#125; for i in J: dic[i] = 0 for j in S: if j in dic: dic[j] += 1 return sum(dic.values()) 3.无重复字符的最长子串难度:中等要求 给定一个字符串，请你找出其中不含有重复字符的 最长子串 的长度。 示例 示例 1:输入: “abcabcbb”输出: 3解释: 因为无重复字符的最长子串是 “abc”，所以其长度为 3。 示例 2:输入: “bbbbb”输出: 1解释: 因为无重复字符的最长子串是 “b”，所以其长度为 1。 示例 3:输入: “pwwkew”输出: 3解释: 因为无重复字符的最长子串是 “wke”，所以其长度为 3。 python代码 思路：维护一个列表，依次读取s中的字符，如果该字符不在列表中，则加入，并更新最大长度，若在列表中，则删除掉列表中前面到该字符的全部元素，并将该字符插入到最后 12345678910111213141516class Solution: def lengthOfLongestSubstring(self, s): """ :type s: str :rtype: int """ max_length = 0 list_str = [] for i in s: if i not in list_str: list_str.append(i) max_length = max_length if max_length &gt; len(list_str) else len(list_str) else: del list_str[:list_str.index(i)+1] list_str.append(i) return max_length 454. 四数相加 II难度:中等思路 给定四个包含整数的数组列表 A , B , C , D ,计算有多少个元组 (i, j, k, l) ，使得 A[i] + B[j] + C[k] + D[l] = 0。为了使问题简单化，所有的 A, B, C, D 具有相同的长度 N，且 0 ≤ N ≤ 500 。所有整数的范围在 -228 到 228 - 1 之间，最终结果不会超过 231 - 1 。 示例 输入:A = [ 1, 2]B = [-2,-1]C = [-1, 2]D = [ 0, 2]输出:2解释:两个元组如下: (0, 0, 0, 1) -&gt; A[0] + B[0] + C[0] + D[1] = 1 + (-2) + (-1) + 2 = 0 (1, 1, 0, 0) -&gt; A[1] + B[1] + C[0] + D[0] = 2 + (-1) + (-1) + 0 = 0 Python 代码 思路：维护一个字典，键为AB各取一个元素的和，值为这个和出现的次数，在CD中各取一个元素求和，去相反数，如果相反数在字典中，则ans加上这个相反数对应的值 12345678910111213141516171819202122232425class Solution: def fourSumCount(self, A, B, C, D): """ :type A: List[int] :type B: List[int] :type C: List[int] :type D: List[int] :rtype: int """ length = len(A) dic_AB = &#123;&#125; ans = 0 for i in range(length): for j in range(length): sum_AB = A[i] + B[j] if sum_AB not in dic_AB: dic_AB[sum_AB] = 0 dic_AB[sum_AB] += 1 for i in range(length): for j in range(length): sum_CD = C[i] + D[j] if -sum_CD in dic_AB: ans += dic_AB[-sum_CD] return ans 347. 前K个高频元素难度:中等要求 给定一个非空的整数数组，返回其中出现频率前 k 高的元素 示例 示例 1:输入: nums = [1,1,1,2,2,3], k = 2输出: [1,2] 示例 2:输入: nums = [1], k = 1输出: [1] 你可以假设给定的 k 总是合理的，且 1 ≤ k ≤ 数组中不相同的元素的个数。 Python代码123456789101112131415161718class Solution: def topKFrequent(self, nums, k): """ :type nums: List[int] :type k: int :rtype: List[int] """ dic = &#123;&#125; for i in nums: if i not in dic: dic[i] = 0 dic[i] += 1 a = sorted(dic.items(), key=lambda item:item[1],reverse=True) # 按值排序，reverse默认为False，为升序，True为降序，返回的是一个列表，列表元素为有序的键值元组 ans = [] for i in range(k): ans.append(a[i][0]) return ans 380. 常数时间插入、删除和获取随机元素难度:中等要求 设计一个支持在平均 时间复杂度 O(1) 下，执行以下操作的数据结构。insert(val)：当元素 val 不存在时，向集合中插入该项。remove(val)：元素 val 存在时，从集合中移除该项。getRandom：随机返回现有集合中的一项。每个元素应该有相同的概率被返回。 示例 // 初始化一个空的集合。RandomizedSet randomSet = new RandomizedSet();// 向集合中插入 1 。返回 true 表示 1 被成功地插入。randomSet.insert(1);// 返回 false ，表示集合中不存在 2 。randomSet.remove(2);// 向集合中插入 2 。返回 true 。集合现在包含 [1,2] 。randomSet.insert(2);// getRandom 应随机返回 1 或 2 。randomSet.getRandom();// 从集合中移除 1 ，返回 true 。集合现在包含 [2] 。randomSet.remove(1);// 2 已在集合中，所以返回 false 。randomSet.insert(2);// 由于 2 是集合中唯一的数字，getRandom 总是返回 2 。randomSet.getRandom(); Python代码 思路：既然是要求在常数时间内的操作，很自然想到要用到数组的序号来操作，这里维护一个字典和一个列表，字典的键为元素，字典值为该元素在列表中的序号 在插入时，字典和列表均插入元素，字典中的值设为列表中元素的序号 在删除时，先在字典中获取待删除元素的序号，在列表中将末尾元素替换至该序号处，在字典中修改替换过来的元素的值，最后pop掉列表最后一个元素，同时删除掉字典中的val 在随机获取时，使用random随机获取一个序号，然后返回列表中对应序号的元素 123456789101112131415161718192021222324252627282930313233343536373839404142434445464748495051525354555657import randomclass RandomizedSet: def __init__(self): """ Initialize your data structure here. """ self.dic = &#123;&#125; self.lst = [] def insert(self, val): """ Inserts a value to the set. Returns true if the set did not already contain the specified element. :type val: int :rtype: bool """ if val not in self.dic: self.lst.append(val) self.dic[val] = len(self.lst) - 1 return True return False def remove(self, val): """ Removes a value from the set. Returns true if the set contained the specified element. :type val: int :rtype: bool """ if val in self.dic: idx = self.dic[val] self.lst[idx] = self.lst[-1] # 与末尾元素交换 self.dic[self.lst[idx]] = idx # 修改字典中交换元素的值 self.lst.pop() del self.dic[val] return True return False def getRandom(self): """ Get a random element from the set. :rtype: int """ idx = random.randint(0, len(self.lst)-1) return self.lst[idx] # Your RandomizedSet object will be instantiated and called as such:# obj = RandomizedSet()# param_1 = obj.insert(val)# param_2 = obj.remove(val)# param_3 = obj.getRandom()]]></content>
      <tags>
        <tag>LeetCode</tag>
        <tag>刷题</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[Python之numpy]]></title>
    <url>%2F2019%2F01%2F26%2FPython%E4%B9%8Bnumpy%2F</url>
    <content type="text"><![CDATA[numpy.randomnumpy.random.rand() numpy.random.rand(d0,d1,…,dn) 根据给定维度生成[0,1)之间的数据，包含0，不包含1 返回值为指定维度的array 1234np.random.rand(3,2)# array([[0.77342926, 0.13062061],# [0.04889282, 0.66763161],# [0.72599757, 0.27826069]]) numpy.random.randn() numpy.random.randn(d0,d1,…,dn) 返回一个或一组样本，具有标准正态分布。 返回值为指定维度的array 1234567np.random.randn() # 没有参数时,返回单个数# -1.3281108519652927np.random.randn(3,2)# array([[ 2.11802305, 2.85387116],# [-0.01947161, -2.09343863],# [-1.96342724, 0.01979386]]) numpy.random.randint() numpy.random.randint(low, high=None, size=None, dtype=’l’) 返回随机整数，范围区间为[low,high），包含low，不包含high dtype为数据类型,默认是np.int 没有high时,默认的是[0,low) 123456np.random.randint(2, size = 10)# array([1, 1, 1, 1, 0, 1, 1, 0, 1, 0])np.random.randint(1, 5, size = (2,4))# array([[3, 1, 2, 2],# [3, 2, 4, 1]]) numpy.random.random_sample()numpy.random.random()numpy.random.ranf()numpy.random.sample() numpy.random.—(size = None) 返回[0.0, 1.0]区间的浮点数 1234567np.random.random_sample()# 0.43535270552664374np.random.ranf((3,2))# array([[0.07730948, 0.19447818],# [0.66644615, 0.41688169],# [0.92430307, 0.78935612]]) numpy.random.choice() numpy.random.choice(a, size = None, replace = True, p = None) a:一位数组或一个int值,一维数组的话就是从数组中随机生成元素,int的话就类似于np.arange(a) size:int值或者int元组,为输出形状 replace:是否可以出现重复值,默认为True,False的话就不允许有重复 p:a中的每个数出现的概率,默认为均匀分布的 返回值为指定维度的array 12345678910111213# 生成大小为3,范围为np.arange(5)np.random.choice(5,3)# array([4, 3, 3])# 生成大小为3,范围为np.arange(5)的非均匀的np.random.choice(5,3,p = [0.1, 0, 0.6, 0.2, 0.1])# array([3, 0, 2])# 由数组生成aa_milne_arr = ['pooh', 'rabbit', 'piglet', 'Christopher']np.random.choice(aa_milne_arr, 5, p=[0.5, 0.1, 0.1, 0.3])# array(['pooh', 'Christopher', 'piglet', 'Christopher', 'pooh'],# dtype='&lt;U11') numpy.random.seed numpy.random.seed(seed = None) seed:指定seed值,将会得到相同的随机结果 1234567np.random.seed(1000)np.random.randint(5)# 3np.random.seed(1000)np.random.randint(5)# 3 参考于: https://docs.scipy.org/doc/numpy-1.15.1/reference/routines.random.html https://www.jianshu.com/p/214798dd8f93 其他 astype操作总是会返回一个新的数组 两个数组之间的运算是对应位置元素的运算;数组与标量的运算是涉及到每一个数组元素的;数组与数组之间的比较是产生一个bool数组 python的list与numpy的array的重要区别:list的切片是产生了一个新的list,而array的切片是产生一view,即对list的切片进行值的修改,不影响原list,但是nrray会影响 1234567891011list1 = [1,2,3,4,5,6]arr1 = np.array(list1)list2 = list1[:3]arr2 = arr1[:3]arr2[0] = 1234list2[0] = 1234# list1:[1,2,3,4,5,6]# list2:[1234, 2, 3]# arr1:array([1234, 2, 3, 4, 5, 6])# arr2:array([1234, 2, 3]) numpy.where() numpy.where(condition[, x, y]) condition:条件,数组的形式,满足时返回x的对应值,不满足时返回y x,y:数组形式 123456x = np.arange(9.).reshape(3, 3)np.where(x&lt;5, x, 0)# array([[0., 1., 2.],# [3., 4., 0.],# [0., 0., 0.]]) numpy.any检测数组中只要有一个ture返回就是true，而numpy.all检测数组中都是true才会返回true 123bools = np.array([False, False, True, False])np.any(bools) # Truenp.all(bools) # False numpy.unique 能返回排好序且不重复的值,相当于Python的sort(set(x)) 123ints = np.array([3, 3, 3, 2, 2, 1, 1, 4, 4])np.unique(ints)# array([1, 2, 3, 4]) numpy.in1d 测试一个数组的值是否在另一个数组里，返回一个布尔数组 123values = np.array([6, 0, 0, 3, 2, 5, 6])np.in1d(values, [2, 3, 6])# array([ True, False, False, True, True, False, True], dtype=bool) numpy.cumsum() numpy.cumsum(a, axis=None, dtype=None, out=None) 返回给定轴上的累加和 a:数组形式 axis:给定轴,默认是一维地去累加 返回一个ndarray 12345678910a = np.array([[1,2,3], [4,5,6]])# array([[1, 2, 3],# [4, 5, 6]])np.cumsum(a)# array([ 1, 3, 6, 10, 15, 21])np.cumsum(a,axis=0)# array([[1, 2, 3],# [5, 7, 9]])]]></content>
      <tags>
        <tag>Python</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[人工智能复习笔记]]></title>
    <url>%2F2019%2F01%2F26%2F%E4%BA%BA%E5%B7%A5%E6%99%BA%E8%83%BD%E5%A4%8D%E4%B9%A0%E7%AC%94%E8%AE%B0%2F</url>
    <content type="text"><![CDATA[四大流派：符号主义(专家系统)，连接主义(人工神经网络)，行为主义(机器人)，统计主义(机器学习)机器学习：监督，半监督，非监督，强化，迁移线性回归 model： loss function： 参数更新： 逻辑回归 model： loss function： 参数更新： 推导为什么不能用均方差代替交叉熵： Cascading logistic regression model：为了解决逻辑回归不能处理异或问题，将多个逻辑回归连接起来，一部分负责特征转换，一部分负责分类，这样就就解决了异或问题；这样的每一个逻辑回归就是神经元，串起来形成了神经网络 如何加快模型训练 1特征缩放，特征归一化 2改变梯度下降方法：随机梯度下降，批量梯度下降 3微调学习率 什么是机器学习对于某类任务T和性能度量P，一个计算机程序被认为可以从经验E中学习是指，通过经验E改进后，它在任务T上由性能度量P衡量的性能有所提升 优化算法损失函数 平方损失函数更适合输出为连续，并且最后一层不含Sigmoid或Softmax激活函数的神经网络；交叉熵损失则更适合二分类或多分类的场景。 均方差： 交叉熵： 凸函数凸函数曲面上任意两点连接而成的线段，其上的任意一点都不会处于该函数曲面的下方 梯度下降 推导： 经典的梯度下降法在每次对模型参数进行更新时，需要遍历所有的训练数据。当M很大时，这需要很大的计算量，耗费很长的计算时间，在实际应用中基本不可行。 随机梯度下降法用单个训练数据即可对模型参数进行一次更新，大大加快了收敛速率。 随机梯度下降法对梯度的估计常常出现偏差，造成目标函数曲线收敛得很不稳定，伴有剧烈波动，有时甚至出现不收敛的情况 为了降低随机梯度的方差，从而使得迭代算法更加稳定，也为了充分利用高度优化的矩阵运算操作，在实际应用中我们会同时处理若干训练数据，该方法被称为小批量梯度下降法。 小批量梯度下降的m通常选取2的幂次时能充分利用矩阵运算操作，先对数据进行随机排序，再顺序挑选m个训练数据，学习率通常先较大，后减小。 AdaGrad方法采用历史梯度平方和，其公式为： RMSProp公式： Adam方法将惯性保持和环境感知这两个优点集于一身。一方面记录了过往梯度与当前梯度的平均，体现了惯性保持，一方面记录了过往梯度平方与当前梯度平方的平均，体现了环境感知。 正则化L1:L2: Dropout Dropout相当于每次迭代都在训练不同结构的神经网络，可以被认为是一种实用的大规模深度神经网络的模型集成算法，减少过拟合风险，增强泛化能力 testing时没有dropout，training时dropout是p%的话，测试时所有的weights要乘以1-p% 过拟合和欠拟合 欠拟合：高bais error，低variance error；过拟合：低bais error，高variance error Bias error ≈ 训练集上的错误率，训练集上的错误率 = avoidable error + unavoidable error，Variance error ≈ 开发集（或测试集）上的表现比训练集上差多少 ， 过拟合就是在训练集上的表现很好，但在测试集和新数据上的表现较差；欠拟合指的是模型在训练和预测时表现都不好的情况。 降低过拟合方法：1获取更多数据2降低模型复杂度3正则化4集成学习方法5提前停止6dropout 降低欠拟合方法：1添加新特征2增加模型复杂度3减小正则化系数4新的激活函数5自适应的学习率6选择合适的损失函数 神经网络 反向传播用以更新模型参数;正向传播用以计算loss 激活函数 sigmoid及其导数： tanh及其导数： softmax及其导数： sigmoid梯度消失的原因 根据其导函数可知，当z很大或很小时，f(z)趋近于0，会造成梯度消失。 Relu相较于sigmoid的优点及局限性 Relu是Maxout的一个特殊情况 优点：1.sigmoid和tanh都需要进行指数的计算，复杂度高，而relu只需要一个阈值就可激活；2.relu的非饱和性可以有效地解决梯度消失的问题；3.relu的单侧抑制提供了网络的稀疏表达能力 局限性：在训练过程中导致神经元死亡的问题，该神经元的梯度永远为0，不再被激活，解决方法是relu的变种形式 神经网络的训练技巧 参数不能全部初始化为0，因为神经元结构相同，输入输出相同，无法训练，需要随机化参数来打破对称性； 如果不进行归一化处理，会致使网络在每一次迭代中都需要拟合不同的数据分布，增大训练的复杂度以及过拟合的风险。归一化的方法是：针对每一批数据，在网络的每一层输入之前增加归一化处理（均值为0，标准差为1），公式为： CNN 计算公式： 为什么选择CNN1.一些特征比整张图片要小很多，不需要看整张图–卷积 2.一些特征会在一张图的不同区域出现–卷积 3.二次抽样不会改变对象–池化 稀疏交互和参数共享稀疏交互的物理意义是先学习局部特征，再将局部特征组合为更复杂更抽象的特征。参数共享的物理意义是卷积层具有平移等变性。 池化包括均值池化和最大值池化，本质是降采样，而且能够保持对平移、伸缩、旋转操作的不变性。 为什么要deep deep-&gt;模块化-&gt;更少的训练数据 类似于决策树，每一层的每一个神经元相当于一个基本的分类器，后面的层的神经元使用前一层的来建立一个新的分类器。复杂的问题模块化 RNN序列数据举例语音识别，情感分析，DNA序列分析，机器翻译等 LSTM 输入门，输出门，遗忘门，输入门控制当前计算的新状态有多大程度更新到记忆单元中，遗忘门控制前一步记忆单元中的信息有多大程度被遗忘掉，输出门控制当前的输出有多大程度取决于当前的记忆单元。 遗忘门接近于1时，实现记忆功能，遗忘门接近于0时，实现的是遗忘功能。 三个门的激活函数为sigmoid，sigmoid函数输出在0~1之间，符合门控的物理定义。在生成候选记忆时，使用tanh，输出在-1~1之间，大多数场景下分布为0为中心相吻合，且在0附近有更大的梯度，收敛更快。 4个输入，1个输出 long short-term memory 可以理解为：比较长的-短时-记忆 LSTM可以解决RNN的梯度消失，但不能解决梯度爆炸 RNN对比CNN等前馈神经网络 一般的前馈神经网络，如CNN，可以捕捉到局部特征，但是序列中的依赖关系很难被学到，而RNN第t层的隐含状态h编码了序列中前t个输入的信息，最后一层的状态h编码了整个序列的信息，RNN具备对序列顺序信息的刻画能力，往往能得到更准确的结果。 RNN梯度消失、梯度爆炸问题 由于预测的误差是沿着神经网络的每一层反向传播的，因此当雅克比矩阵的最大特征值大于1时，随着离输出越来越远，每层的梯度大小会呈指数增长，导致梯度爆炸；反之，若雅克比矩阵的最大特征值小于1，梯度的大小会呈指数缩小，产生梯度消失。 梯度爆炸的问题可以通过梯度裁剪来缓解，即当梯度的范式大于某个给定值 时，对梯度进行等比收缩。 LSTM及GRU通过加入门控机制，很大程度上弥补了梯度消失带来的损失 迁移学习概念 把已学训练好的模型参数迁移到新的模型来帮助新模型训练，通过迁移学习我们可以将已经学到的模型参数（也可理解为模型学到的知识）通过某种方式来分享给新模型从而加快并优化模型的学习效率不用像大多数网络那样从零学习。 应用场景 同样的任务，不同的分布，或者不同的任务，同样的分布 源数据：有标签；目标数据：有标签(很少)；比如：源数据为许多人的语音，目标数据为特定用户的语音；fine-tuning，multitask lear 源数据：有标签；目标数据：无标签；比如：源数据为mnist，目标数据为一些不带标签的数字图像；domain-adversarial training,zero-shot learining 源数据：无标签；目标数据：有标签；self-taught learning 源数据：无标签；目标数据：无标签；self-taught clustering]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[机器学习复习笔记]]></title>
    <url>%2F2019%2F01%2F25%2F%E6%9C%BA%E5%99%A8%E5%AD%A6%E4%B9%A0%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[SVM线性SVM loss: hinge loss 凸函数 $l(f(x^n), y^n) = max(0, 1-y^nf(x^n))$, 更为常见的写法是用 $y^nf(x) \geq 1-e^n$, 这里的 $e^n$ 就是松弛变量 边界最宽的 Large-Margin 离边界最近的点就是支撑向量 二次规划(凸优化) 遗传算法 种群-&gt;幸运轮-&gt;存活者-&gt;配对重组变异-&gt;种群-&gt;… 遗传算子:配对 重组 变异 三个算子 算法实现 初始种群产生 个体存活规则:适应函数计算个体的适用度,然后根据概率选出存活的个体 配对(选择)算子 个体任意配对,未考虑适应度高的个体更具有吸引力 按适应度排序,然后相邻的两两配对 概率选择,保留了高质量个体与低质量个体配对的可能,增加了个体的多样性,但是最终优秀个体可能会消失,解决办法是一部分最优秀的个体直接复制到下一代,不经过重组变异 重组(交叉) 单点交叉 两点交叉 均匀交叉 实际80%重组,20%直接复制到下一代 变异:随机选择一位求反,只有配对和交叉,就不存在新个体,会陷入局部最优解的 KNN选择 生成种群:对d维特征进行二进制编码,包含该特征即设为1 适应函数: $f=1/(C_1E_R + C_2N_A)$ 其中$E_R$是分类错误的样本数. $N_A$ 是保留属性的数目. $C_1C_2$ 是可调的参数 选择算子:个体存活的概率为: $P = f(x_i) / \sum f(x_i) $ 重组算子:两点交叉 变异:随机一位取反 确定最优属性:每次保留$y_i$个个体,n次运行后暴露的个体数为$\sum y_i$,统计每个属性在其中出现的频数,降序排列,即可得到最优属性 聚类 定义:将大量无标注的数据集,按数据的内在相似性将数据集划分成多个类别,使得类别内相似性较大,类别间相似性较小 对m个样本,构造k个簇,每个簇至少包含一个对象,每个对象仅属于一个簇 最短距离法: 定义距离计算方法 计算初始样本两两之间的距离,构造距离矩阵 根据距离矩阵,合并最小值对应得到两个样本,形成新样本 更新距离矩阵,重复上面的步骤 重心聚类法:计算每个簇的重心,重心可取样本的平均值,得到一个距离矩阵,后续同上 动态聚类法:先划分成若干个簇,计算所有样本到簇重心的距离,取最小距离作为该样本所属的簇,再更新簇的重心,直到所有的样本所属簇不再变化 k-means算法 对m个样本选取k个点作为中心,分别计算每个样本到这k个点的距离,并把该样本加入到最近距离的簇,所有样本加入到k个簇后,更新k个中心点,再重复,直至不再变化 k-means++算法 在k-means算法的基础之上,在初始生成k个初始点之前,对所有样本进行一次计算,使得初始中心点的距离尽可能远,这样可以减少迭代次数 先随机选择一个点作为第一个中心点,计算所有样本到该点的距离,选择距离最远的作为第二个样本点,依次选出k个 如何选择k值 k值越大,损失越小,k代表了模型的复杂度,模型越复杂,对训练集拟合的越好,但是引起过拟合 轮廓分析:$$s = \frac{b-a}{max(a,b)}$$其中a表示一个类别内所有数据到聚类中心的距离平方和,b表示数据到其他最近聚类中心的距离平方和,s=1时表示达到了最佳状态,s=-1表示最差状态,s越大越好 k-means总结 优点:当簇接近于高斯分布时效果较好;简单快捷; 缺点:必须先给出k值,对初始值敏感;不适合大小差别很大的簇;对噪声和孤立点敏感; LVQ(学习向量量化) 与k-means不同的是,假设样本带有标签 关键点在于中心点的更新,如果$x_i$与中心点的类别相同,则中心点向$x_i$靠拢,反之远离 密度聚类 初始化核心对象集合 计算每个样本的$\epsilon$-领域,将是核心对象的样本加入到集合中 以任何一个核心对象出发,找出由其密度可达的簇$C_i$ 直到集合内所有核心对象都被访问过 集成学习 Bagging 从D中创建T个训练子集 $D_1,D_2,D_3,…$ 为每一个$D_i$ 引入一个基学习器 $h_i$ 对回归问题使用简单平均;对分类问题使用简单投票 时间复杂度:$T(O(m)+O(n)$,$O(m)$是采样和投票的时间复杂度,$O(n)$是基学习器的计算复杂度 优点:泛化能力强;缺点:训练误差大 随机森林 以决策树为基学习器 分类问题,最终结果等于决策树输出次数最多的类别 回归问题,最终结果等于决策树输出结果的平均值 一颗决策树容易出错,但是多棵树同时犯错的概率就会很低 首先为每棵树创建训练子集;随机挑选部分属性构成候选属性子集,在这个子集中产生最优的属性划分;随机构成一个阈值集合 Schapire提升 bagging方法的严重缺陷在于各基学习器之间的关联性很弱 问题:用一个三元组和其他两个基分类器组成新的三元组后，集成分类器的性能就不会有多大的提升。 解决方法是三元组均由三个基分类器组成 Adaboosting 生成训练子集:生成第一个训练子集时,每个样本被选中的几率相同,并由此训练得到第一个分类器,然后减小被错误分类样本的概率,增大错分样本选中的概率 集成时,加大分类误差小的分类器的权重,减小分类误差大的分类器的权重 加法模型: $H(x) = \sum{a_ih_i(x)} $ 决策规则: $H(x) = sign( \sum{a_ih_i(x)})$ 贝叶斯 先验概率可以认为就是频率,后验概率就是条件概率,后验概率比先验概率更可信 贝叶斯公式:$$P(C_i|x) = \frac{P(x|C_i)*P(C_i)}{P(x)}$$ 贝叶斯假设,即属性之间是相互独立的:$$P(x|C_j) = \prod_{i=1}^dP(x_i|C_j)$$ 由此得到贝叶斯分类器:$$h(x) = argmax(P(C_j) * \prod_{i=1}^dP(x_i|C_j )$$ 属性之间不是相互独立的怎么办 忽略其影响 不能忽略时,可采用的方法有:去掉该属性,用其他属性替换之,或者采用别的分类方法 朴素贝叶斯算法 计算先验概率和条件概率,即各个标签的概率以及各个标签下各个特征的条件概率 对于给定的实例,计算$P(C_j) * \prod_{i=1}^dP(x_i|C_j )$ 选取其中的最大值,作为该实例的类别 平滑处理(拉普拉斯平滑) 先验概率,分母加上类别数,分子加1 条件概率,分母加上该属性的可能取值数,分子加一 KNN 三要素:k值选择,距离度量,分类决策规则 使用的距离:欧氏距离 k值得选择:k值减小意味着模型更复杂,容易过拟合;k值增加意味着模型变简单;实际k一般选一个较小的奇数,或者采用交叉验证的方式找到最好的k 分类决策规则:多数投票表决 kd树: 加权最近邻:距离近的权重大,距离远的权重小 具体的:将k个近邻点的距离从小到大排序:$d_1, d_2, d_3,…d_k$ 则$$\begin{cases}\frac{d_k-d_i}{d_k-d_1},d_k ≠ d_1\1, d_k=d_1\end{cases}$$ 标准化属性尺度:$x = (x - min)/(max-min)$ 线性回归 最小二乘法:基于方差最小化进行模型求解的方法就是最小二乘法 梯度下降 逻辑回归 sigmoid函数: $z = wx, f = \frac{1}{1+e^{-z}}, f’ = f*(1-f)$ 决策树 以信息熵为度量,构造一棵熵值下降最快的树,到叶子节点处的熵值为0 信息量 $I(x) = -log_2p(x)$, 事件发生的概率越小,信息量越大,对于确定性事件(p=1),信息量为0 熵是平均信息量:$H(x) = -\sum p(x)log_2p(x)$,则对于两点分布的来说:$H(x) = -p(x)log_2p(x)-(1-p(x))log(1-p(x))$ 条件熵:$H(Y|X) = \sum_{i=1}^np_iH(Y|X=x_i)$ 熵和条件熵的概率值是由相对频率估算得到的话,叫做经验熵和条件经验熵 特征A对训练数据集D的信息增益:g(D,A) = H(D) - H(D|A) 经验熵$H(D)$表示对数据集D分类的不确定性,经验条件熵$H(D|A)$表示在特征A的给定条件下对数据集D分类的不确定性,他们的差值即信息增益表示特征A对数据集D的分类不确定性的减少程度,信息增益大的特征具有更强的分类能力 信息增益划分的缺点是太偏好取值数目较多的属性,C4.5采用信息增益率进行划分 信息增益率:$g_R(D,A) = \frac{g(D,A)}{H_A(D)}$, 其中$H_A(D)$为特征A的经验熵 老师课件中提到:C4.5不是直接选取信息增益率最大的属性进行划分,而是从信息增益高于平均值属性中选取信息增益率最大的(统计学习方法一书中并没有这么说,就是C4.5根据信息增益率大小划分) 剪枝:为了降低过拟合,用叶子节点代替子树,会引起部分训练样例的错误分类,但是我们关心的是模型的泛化能力,而不是在训练集上的100%正确率 误差估计:错误率为$E=\frac{e+1}{n+m}$其中e是错误分类的样本数,n是达到测试节点t的样本总数,为避免n过小进行了修正 用测试样例比较剪枝前后的准确率,准确率提高则剪枝,反之不剪 预剪枝:在决策树生成的过程中,对每个节点划分前先进行估算,若划分后没有泛化能力的提升,则将该节点标记为叶子节点 后剪枝:先从训练集生成一棵完整的决策树,然后自底向上地对非叶子节点进行考察,若该节点换成叶子节点可以提高泛化能力,则剪枝 CART树 对回归树用平方误差最小化,对分类树用基尼指数最小化 分类树划分属性时采用的是基尼指数:$Gini(D) = \sum_{k=1}^Kp_k(1-p_k) = 1-\sum_{k=1}^Kp_k^2$,对于二分类问题就是:$Gini(p)=2p(1-p)$ 若样本D根据特征A的取值被划分为D1和D2两部分,则在A条件下,D的基尼指数为:$Gini(D,A) = \frac{|D_1|}{|D|}Gini(D_1)+\frac{|D_2|}{D}Gini(D_2)$ 选择基尼指数最小的属性作为最优划分属性 人工神经网络 所谓的M-P模型就是按照生物神经元的结构和工作原理构造出来的一个抽象和简化了的模型，它实际上就是对单个神经元的一种建模 BP算法的目标:最小化网络误差 解决过拟合: 采用早停策略,当训练误差降低而测试误差升高时,停止训练 正则化 跳出局部最小值: 随机梯度下降 以多种不同的参数初始化,训练后误差最小的作为最终结果 模拟退火,在每一步都以一定概率接受比当前解更差的结果]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
  <entry>
    <title><![CDATA[算法复习笔记]]></title>
    <url>%2F2019%2F01%2F25%2F%E7%AE%97%E6%B3%95%E5%A4%8D%E4%B9%A0%2F</url>
    <content type="text"><![CDATA[顺序表的静态定义12345678910111213// 第一种，定义一个数组int Sqlist[100];// 第二种，定义一个宏来描述大小#define List_size 100int Sqlist[List_size];// 第三种，定义一个结构体#define List_size 100typedef struct&#123; int elem[List_size]; int len; // 描述实际长度&#125;Sqlist; 顺序表动态定义1234567891011121314151617// 利用指针#define List_size 100typedef struct&#123; int *elem; int len; // 描述实际长度 int listsize; //当前分配空间大小&#125;Sqlist;int InitSqlist(Sqlist *L)&#123; L-&gt;elem = (int*)malloc(List_size * sizeof(int)); if(L-&gt;elem == NULL)&#123; exit(EXIT_FAILURE); &#125; L-&gt;len = 0; L-&gt;listsize = List_size; return 1;&#125; 顺序表插入操作12345678910111213141516// 在第i个位置插入eint listInsert_Sq(Sqlist &amp;L, int i, int e)&#123; // 插入位置合理 if(i&lt;1 || i&gt;L.len+1) return 0; // 出现上溢 if(L.len &gt;= L.listsize)&#123; newbase = (int*)realloc(L.elem, (L.listsize + ListIncerament) * sizeof(int)); if (newbase == NULL) exit(OVERFLOW); L.elem = newbase; L.listsize = L.listsize + ListIncerament; &#125; for(int j = L.len; j&gt;=i; j--) L.elem[j] = L.elem[j-1]; L.elem[i-1] = e; L.len++; return 1;&#125; 顺序表的删除操作1234567// 删除第i个元素int listDelete_Sq(Sqlist &amp;L, int i)&#123; if(i&lt;1 || i&gt; L.len) return 0; for(int j = i; j&lt;L.len; j++) L.elem[j-1] = L.elem[j]; L.len--; return 1;&#125; 顺序表优缺点 优点：随机存取 缺点：空间利用率低；静态的不可扩充；动态的反复扩充开销大；插入删除需要移动大量元素； 适用于输入数据大小已知，无需太多动态操作的应用 单链表的定义123456struct Node&#123; int data; struct Node *next;&#125;;typedef struct Node *Link;Link head; 有头节点的单链表插入算法1234567891011121314151617// 在第i个位置插入元素eint listInsert(Link &amp;L, int i, int e)&#123; Link p = L; int j = 0; while(p!= NULL &amp;&amp; j &lt; i-1)&#123; p = p-&gt;next; j++; &#125; if(p == NULL || j&gt;i-1) return 0; struct Node s; s = (Link)malloc(sizeof(Node)); if(s == NULL) exit(OVERFLOW); s-&gt;data = e; s-&gt;next = p-&gt;next; p-&gt;next = s; return 1;&#125; 无头节点的单链表插入算法12345678910111213141516171819202122int listInsert(Link &amp;L, int i, int e)&#123; if(i == 1)&#123; struct Node s; s = (Link)malloc(sizeof(Node)); s-&gt;data = e; s-&gt;next = L; L = s; &#125;else&#123; Link p = L; int j = 1; while(p != NULL &amp;&amp; j&lt;i-1)&#123; p = p-&gt;next; j++; &#125; if(p == NULL || j&gt;i-1) return 0; struct Node s; s = (Link)malloc(sizeof(Node)); s-&gt;data = e; s-&gt;next = p-&gt;next; p-&gt;next = s; &#125; return 1; 头插法1234s = (Link)malloc(sizeof(Node));scanf(&amp;s-&gt;data);s-&gt;next = L-&gt;next;L-&gt;next = s; 尾插法1234p-&gt;next = (Link)malloc(sizeof(Node));p = p-&gt;next;scanf(&amp;p-&gt;data);p-&gt;next = NULL; 双向链表定义12345678910struct Node&#123; int data; struct Node *next; struct Node *prev;&#125;;typedef struct Node *Link;typedef struct&#123; Link head, tail; int len;&#125;DLinkList; 顺序栈的定义12345678struct stack&#123; int *base; int stack_size; int min_stack; int max_stack; int top;&#125;;typedef struct stack Stack; 初始化栈12345678910111213Stack *CreateStack(int how_many)&#123; Stack *pstk; assert(how_many&gt;0); pstk = (Stack*)malloc(sizeof(Stack)); if(pstk == NULL) return(NULL); pstk-&gt;stack_size = how_many; pstk-&gt;base = (int*)malloc(how_many * sizeof(int)); if(pstk-&gt;base == NULL) return(NULL); pstk-&gt;min_stack = 0; pstk-&gt;max_stack = how_many-1; pstk-&gt;top = -1; return(pstk);&#125; 销毁栈12345void DestroyStack(Stack *this_stack)&#123; this_stack-&gt;top = -1; free(this_stack-&gt;base); this_stack-&gt;base = NULL;&#125; 随机访问栈元素123456// which_elem 表示待查元素与栈顶的间隔int* viewElem(Stack *this_stack, int which_elem)&#123; if(this_stack-&gt;top == -1) return(NULL); if(this_stack-&gt;top - whick_elem &lt; 0 ) return(NULL); return(&amp;(this_stack-&gt;base[this_stack-&gt;top-which_elem]));&#125; 入栈1234567int pushElem(Stack *this_stack, int *to_push)&#123; // 判断栈满 if(this_stack-&gt;top == this_stack-&gt;max_stack) return(0); this_stack-&gt;top += 1; memmove(&amp;(this_stack-&gt;base[this_stack-&gt;top]), to_push, sizeof(int)); return(1);&#125; 出栈1234567int popElem(Stack *this_stack, int *dest)&#123; // 判断栈空 if(this_stack-&gt;top == -1) return(0); memmove(dest, &amp;(this_stack-&gt;base[this_stack-&gt;top]), sizeof(int)); this_stack-&gt;top -= 1; return(1);&#125; 推荐适用顺序栈 实现简单 栈的受限操作正好屏蔽了顺序表的弱势：插入和删除都是在同一端进行的 栈、队列对比 栈：栈顶（top）允许插入和删除的一端；栈底（base/bottom）表头端 队列：队头（front）允许删除的一端；队尾（rear）允许插入的一端 队列的应用：操作系统的作业排队顺序队列定义1234567#define Max_QSize 100typedef struct&#123; int *base; int front; int rear;&#125; queue_struct;queue_struct SqQueue; 顺序对列的入队出队以及队空队满 入队：若未满，Q.rear++ 出队：若未空，Q.front++ 队空：Q.front == Q.rear 队满：Q.rear == Max_QSize 问题是存在假上溢 构建循环队列，解决假上溢的问题 入队：若未满，Q.rear = (Q.rear+1)% Max_QSize 出队：若不空，Q.front = (Q.front+1)% Max_QSize 队空：Q.rear == Q.front 队满：Q.rear == Q.front 问题是对空队满判断一样 解决：少一个元素空间，堆满改为：(Q.rear+1)% Max_QSize == Q.front 链队列 队头用链表的头指针表示 队尾用链表的尾指针表示 无队满问题 队空：Q.rear == Q.front Hash表定义 用动态定义的顺序表来定义hash表 1234567#define Table_Size 100struct HashTable_struct&#123; int *elem; int len; int tableSize;&#125;;typedef struct HashTable_struct hash_table; 冲突处理办法 线性再散列 非线性再散列 外部拉链法 线性再散列的缺点是1.不能删除表中元素，解决办法是把用过的槽标记为无效2.当表被填满时性能显著下降 再散列法的优点是1容易进行动态编码2负载因子较低且不太可能删除元素时速度快，负载因子大于0.5时不建议使用 外部拉链法可以容纳的元素取决于内存的大小，而再散列法取决于表的大小 外部拉链法平均查找时间 = 链表长度/2 + 1 外部拉链法的缺点是需要更多的存储空间 二分查找123456789101112131415161718192021222324252627// 确定在有序整数序列x中，整数t第一次出现的位置int binarySearch(int t)&#123; int l,u,m; l = 0; u = n-1; for(;;)&#123; if(l&gt;u) return -1; m = (l+u)/2; if(x[m] &lt; t) l = m+1; else if(x[m] == t) return m; else u = m-1; &#125;&#125;// 优化1int binarySearch1(int t)&#123; int l,u,m; l = 0; u = n-1; while(l&lt;=u)&#123; m = (l+u)/2; if(x[m]&lt;t) l = m+1; else if(x[m] == t) return m; else u = m-1; &#125; return -1;&#125; 代码调优通用法则 利用等价的代数表达式，比如模运算的优化 利用宏替换函数，但有时会起反作用 利用哨兵合并测试条件 展开循环 高速缓存需经常处理的数据 1234567891011121314151617// 整数取模// 优化前k = (j + r) % n;// 优化后k = j + r;while(k &gt; n) k -= n; // 利用宏// 优化前float max(float a, float b)&#123; return a&gt;b?a:b;&#125;// 优化后#define max(a,b) ((a)&gt;(b)?(a):(b)); 朴素的模式匹配算法 时间复杂度：设n为主串长度，m为子串长度，O((n-m+1)*m) 最好情况：O(m) 平均情况：每次匹配不成功都是在子串的首字符处，O(m+n) 最坏情况：O((n-m+1)*m) 缺点是需要回溯 KMP算法 近似时间复杂度为O(m+n), O(n)为比较的时间，O(m)为计算next数组的时间 目标串中不存在回溯 目标串中每个字符会比较1～2次 字符串的下标是从1开始 仅当模式串与目标串存在许多部分匹配时，KMP才比朴素模式匹配有优势 若每一次不匹配都发生在第一个字符，KMP会退化为朴素模式匹配 BM算法 从右往左 坏字符：子串从右开始标，第一个为0，每种字符只标一次，大小为到最右的距离，其他为整个子串长度 好后缀：子串从右开始标，第一个为1，完整的好后缀从右往左找重复的，不完整的好后缀从头往后找重复的，大小等于好后缀长度加上到重复的距离 通常模式串越长，BM算法越快，因为每一次失败的匹配，BM都能够使用这些信息来排除尽可能多的无法匹配的位置 二叉树的性质 第i层最多 $2^{i-1}$ 个节点 深度为k的二叉树最多 $2^{k+1}-1$个节点 $n_0 = n_2 + 1$ 二叉树顺序存储定义12345678910// 定义1#define MAX_TREE_SIZE 100ElemType SqBiTree[MAX_TREE_SIZE];// 定义2#define MAX_TREE_SIZE 100typedef struct&#123; ElemType elem[MAX_TREE_SIZE+1]; int len;&#125;SqBiTree; 二叉链表定义1234typedef struct&#123; ElemType data; struct BiTNode *lchild, *rchild;&#125;BiTNode, *BiTree; 二叉排序树 中序遍历二叉排序树，可以得到一个关键字的有序序列 删除操作：用中序遍历的直接后继或直接前驱进行替换 构造二叉排序树的形状依赖于数据项，且依赖于它们加载的顺序 为解决二叉排序树的失衡问题，提出了AVL树，红黑树，伸展树 AVL树 发生失衡时，找到不平衡的最小子树根结点开始调节 红黑树 2-3-4树 插入时，遇到4节点就要分 伸展树 优点：最近使用过的数据比未使用的数据更快被访问 查找：若n在树t上，则以n为根结点进行重排；若n不在树t上，则将查找n的过程中第一个非空叶子节点作为根结点进行重排；最后判断根结点与n是否相同 插入：根据重排后的根结点与待插入的n之间的关系，决定n插入的位置 删除：重排后，待删除的n位于根节点处，删除n后，若左孩子非空，则用中序遍历的前驱代替n，若左孩子为空，则用右孩子代替 自底向上：待访问节点c，父节点p，祖父节点g 若c无祖父，直接在cp之间旋转 若cpg之间为LL或RR的关系，则pg先旋转，然后pc旋转 若cpg之间为LR或RL的关系，则cp先旋转，然后cg旋转 B树/B+树 如何快速检索内存中的数据 二叉排序树，hash表，二分查找 如何快速检索磁盘上的海量数据 减少磁盘IO次数和提高内存检索效率 B和B+区别 B+的叶子节点包含了全部关键字信息以及指向这些关键字的指针，叶子节点本身也以关键字大小自小而大顺序连接 所有非终端节点可以看做索引部分，节点中含有其子树最大或最小的关键字 B+树的特点 B+树不是树 两部分组成 索引块，指针指向块号 数据块，指针指向记录地址 B+树的查找 顺序查找（横向） 随机查找（纵向） B+树的查询效率是常数，B+树的查询效率对于某建成的树是固定的，而B树是与键在树中的位置有关的 应用场景：磁盘上海量数据的检索 Trie树 特性： 1 根节点不含字符，根节点外每个节点含有一个字符 2 从根节点到某一节点路径上经过的字符连接起来为该节点对应的字符串 3 每个节点的子节点都不相同 12345678// Trie的定义const int kind = 26typedef struct Treenode&#123; int count; bool isColored; TrieNode *next[kind];&#125;TrieNode;TrieNode *root; 典型应用：用于统计和排序大量的字符串，所以经常被搜索引擎用于文本词频统计 查找某个字符串操作的复杂度为O(N),其中N为字符串长度，空间换时间 优点：利用字符串的公共前缀来节省存储空间，最大限度减少无谓的比较，查询效率比hash高 中文字典树：Trie树的每个子节点用hash存储 排序 通常排序算法都设计为用于数组，非常适合链表的排序方法有：插入排序和快排 内部排序：把待排的所有记录都加载进内存，然后对内存中的这些记录进行排序 外部排序：待排序的记录无法一次性加载到内存中，因此每次加载部分记录，并进行内部排序 基于比较的排序算法：平均复杂度≥O(NlogN) 交换：冒泡，快排 选择：简单选择，堆排序 插入：直接插入，折半插入，希尔排序 基于某种映射的排序，平均时间复杂度为线性级别：桶排序，基数排序 冒泡 连续扫描待排序的记录 每趟扫描都把最大的记录移动到序列的尾部 若某趟扫描没有任何交换，则表明是有序的了 时间复杂度：平均：O($N^2$),最坏：O($N^2$),最好：O(N) 空间复杂度：交换时需要一个辅助空间temp，所以是O(1) 基于数组的冒泡排序的基本操作是比较和交换 优点是：简单，容易实现；对几乎有序的记录排序的时间开销为O(N) 由相邻的记录进行交换，所有是稳定的 12345678910111213141516171819// 外层循环控制执行的趟数，内层循环控制在一趟中相邻记录间的比较和交换void BubbleSort(Element **Array, int N, CompFunc Compare)&#123; int limit; for(limit = N-1; limit&gt;0; limit--)&#123; int j, swapped; swapped = 0; for(j = 0; j&lt;limit; j++)&#123; if(Compare(Array[j], Array[j+1])&gt;0)&#123; Element *temp; temp = Array[j]; Array[j] = Array[j+1]; Array[j+1] = temp; swapped = 1 &#125; &#125; if(!swapped) break; &#125;&#125; 简单选择排序 连续扫描序列A，不断从待排记录中选出最小的记录放到已排序记录序列的后面，直到n个记录全部插入到已排序序列中 不稳定的：比如3，3，3**，2，在第一遍过后就会变为2，3，3** 对于长度是N的数组，选择排序需要大约N2/2次比较和N次交换 总的来说时间复杂度是T(N) = O($N^2$) 空间复杂度是：交换时需要辅助空间temp，O(1) 12345678910111213141516171819// 外循环用于控制排序的次数，内循环用于查找待排记录中关键字最小的记录void SelectSort(Element **Array, int N, CompFunc Compare)&#123; int i, j, cnt; for(i = 0; i&lt;N-1; i++)&#123; Element *temp; temp = Array[i]; cnt = i; for(j = i+1; i&lt;N; j++ )&#123; if(Compare(temp, Array[j])&gt;0)&#123; temp = Array[j]; cnt = j; &#125; &#125; if(cnt != i)&#123; Array[cnt] = Array[i]; Array[i] = temp; &#125; &#125;&#125; 直接插入排序 逐个处理待排序的记录，将每个记录与前面已经排好的记录序列进行比较，并将其插入到合适的位置 对于已经排序好的序列，从后往前进行扫描 空间复杂度：只需要一个temp，所以O(1) 稳定性：稳定 时间复杂度：外循环始终n-1次，最好情况即为初始是正序的，时间复杂度为：O(n),最坏情况为逆序序列，时间复杂度为O($N^2$),平均情况为O($N^2$) 越接近于有序，该算法效率越高 基于数组的操作：比较和半交换(移位) 优点：对几乎有序的时间开销为O(n);可以用于优化快排 改进：折半插入排序，直接插入排序的查找插入位置采用折半查找的方法实现 12345678910111213141516void InsertSort(Element **Array, int N, CompFunc Compare)&#123; int step; for(step = 1; step&lt;N; step++)&#123; int i; Element *temp; temp = Array[step]; for(i = step-1; i&gt;=0; i--)&#123; if(Compare(Array[i], temp)&gt;0)&#123; Array[i+1] = Array[i]]; &#125; else break; &#125; Array[i+1] = temp; &#125;&#125; 希尔排序 直接插入的改进版 直接插入排序的问题：每次扫描序列，智能确定一个目标的合法位置 思想：先分割成若干小组，分别在组内进行直接插入排序，待基本有序后，再对全体进行一次直接插入排序，每个组内记录间隔h，h稳定递减，h最后一个取值为1 改进的出发点是：直接插入排序对几乎有序的记录排序时间开销为O(N) 时间复杂度为：O($n^{1.25}$) 不稳定 1234567891011121314151617181920void ShellSort(Element **Array, int N, CompFunc Compare)&#123; int step, h; for(h = 1; h&lt;= N/9; h = 3*h+1) ; // h = 1,4,13,40... for(;h&gt;0;h/=3)&#123; //h = ...,40,13,4,1 for(step = h; step&lt;N; step++)&#123; int i; Element *temp; temp = Array[step]; for(i = step-h; i&gt;0; i-=h)&#123; if(Compare(temp, Array[i])&lt;0)&#123; Array[i+h] = Array[i] &#125; else break; &#125; Array[i+h] = temp; &#125; &#125;&#125; 快速排序 分治法:将待排数组分成两个小部分,分别进行递归快排 思想: 若待排的数组中只有一个元素,则退出 否则选择一个元素作为基准 将待排的数组按该元素划分成两个数组A1和A2,其中A1中的元素都小于等于该元素,A2中的元素都大于等于该元素 对A1进行快排 对A2进行快排 12345678910111213141516171819// 算法实现1// 单向划分 &amp; 将数组的首元素作为基准// 性能分析:对于随机数组:T(n) = O(nlogn),栈深度:O(logn)// 因为每层递归都执行了O(n)次比较,总计O(logn)次递归// 对于非随机数组:不管是相同的元素还是升降序的,T(n2)void qsort1(int l, int u)&#123; int i,m; if(l&gt;=u) return; m = l; // m表示存放小于基准元素的子数组A1的最末元素的下标,A1采用尾插法来添加新的元素 for(i = l+1; i&lt;=u; i++)&#123; if(x[i]&lt;x[l])&#123; swap(++m, i); &#125; &#125; swap(l, m); qsort1(l, m-1); qsort1(m+1, u);&#125; 12345678910111213141516171819202122232425// 算法实现2// 双向划分 &amp; 将数组首元素作为基准// i右移过小元素,遇到大元素时停止// j左移过大元素,遇到小元素时停止// 若i,j不交叉,则交换两下标对应的元素// 性能分析:对于随机顺序的数组:T(n) = O(nlogn),栈深度:O(logn)// 对于非随机的数组:若果是相同的元素:T(n) = O(nlogn);如果是升降序的:T(n)=O(n2)void qsort3(int l, int u)&#123; int i, j; DType t; if(l &gt;= u) return; t = x[l]; i = l; j = u+1; for(;;)&#123; do i++;while(i&lt;=u &amp;&amp; x[i]&lt;t); do j--;while(x[j]&gt;t); // 这里不使用等号是怕遇到所有元素都相同时,时间复杂度降到O(n2) if(i&gt;j) break; swap(i, j); &#125; swap(l, j); qsort3(l, j-1); qsort3(j+1, u);&#125; 1234567891011121314151617181920212223// 双向划分 &amp; 将随机元素作为基准// 优点是对于小的子数组,可采用插入排序的方法void qsort4(int l , int u)&#123; int i, j; DType t, temp; if(u-l &lt; cutoff) return; swap(l, randint(l,u)); t = x[l]; i = l; j = u+1; for(;;)&#123; do i++; while(i&lt;=u &amp;&amp; x[i]&lt;t); do j--; while(x[j]&gt;t); if(i&gt;j) break; temp = x[i]; x[i] = x[j]; x[j] = temp; &#125; swap(l, j); qsort4(l, j-1); qsort4(j+1, u);&#125; 其他改进提速方法 改进栈利用率,大的子数组改用迭代循环,小的子数组仍递归快排 利用static,减少栈空间需求 不使用显式的数组索引,改用等价的指针,以保证快排的稳定性 总结 系统自带的sort函数能满足需求,则不需要编写代码 元素个数较少时,可以考虑直接插入排序 元素个数较大时,可以考虑编码实现的快排 堆排序 不存在最坏情况 T= O(nlogn) 和希尔排序一样,是良好的通用排序算法 siftup(n) 在堆的尾部插入新元素后,需要重新获取堆的性质 siftdown(1, n) 用新元素替换堆中的根后,需要重新获取堆的性质 两个阶段: 建立大根堆 依次提取大根堆的根节点,从左到右建立最终的升序序列 12345678910111213141516171819202122232425262728293031323334void hsort1()&#123; int i; x--; for(i = 2; i&lt;=n; i++) siftup(i); for(i = n; i&gt;=2; i--)&#123; swap(1, i); siftdown1(1, i-1); &#125; x++;&#125;void siftup(int u)&#123; int i, p; i = u; for(;;)&#123; if(i == 1) break; p = i/2; if(x[p]&gt;=x[i]) break; swap(p, i); i = p; &#125;&#125;void siftdown1(int l, int u)&#123; int i,c; i = l; for(;;)&#123; c = 2 * i; if(c&gt;u) break; if(c+1 &lt;= u &amp;&amp; x[c+1]&gt;x[c]) c++; if(x[i]&gt;x[c]) break; swap(i,c); i =c; &#125;&#125; 后缀数组 将文本中所有字符保存到字符数组和后缀数组中 对后缀数组进行排序,将相似的元素聚集在一起 扫描后缀数组,比较相邻后缀数组中的相邻元素,找出最长的重复字符串 12345678910111213141516171819202122232425262728int main()&#123; int i, ch,n=0,maxi,maxlen=-1; // a[n]为后缀数组,c[n]为字符数组 while((ch = getchar())!= EOF)&#123; a[n] = &amp;c[n]; c[n++] = ch; &#125; c[n] = 0; // 按字符重新排序 qsort(a,n,sizeof(char*),pstrcmp); // 统计最长重复子串 for(i=0;i&lt;n-1;i++) if(comlen(a[i], a[i+1])&gt;maxlen)&#123; maxlen = comlen(a[i], a[i+1]); maxi = i; &#125; printf("%.*s\n", maxlen, a[maxi]); return 0;&#125;int pstrcmp(char **p, char **q)&#123; return strcmp(*p, *q);&#125;int comlen(char *p, char *q)&#123; int i=0; while(*p &amp;&amp; (*p++ == *q++)) i++; return i;&#125; 海量数据的处理统计 双层桶划分+Trie树/hash表/红黑树 将大文件划分成若干个小文件,利用hash技术 对每个小文件进行词频统计 合并各个小文件的结果 云计算架构 线性结构+直接排序法 顺序表存储,归并排序进行排序,拍完再遍历统计词频 理论上排序的时间复杂度是O(nlogn),遍历的时间复杂度是O(n),所以总体的是时间复杂度是O(nlogn) 实际上每个 单词不止I/O一次,I/O属于耗时操作,所以实际上的时间复杂度远比理论上的大 总结: 如果海量数据无法一次性在内存中处理,可以划分成多个可以在内存中处理的数据区域 再用Trie树/hash表统计 排序 顺序表+直接排序法 先外部排序,比如归并排序,时间复杂度O(nlogn) 对排序完的进行遍历,统计次数,O(n) 总体的时间复杂度就是O(nlogn) 利用hash表 key为字串,value为出现次数 时间复杂度为:O(n * 每个槽上的线性表平均长度) 相比较上一个,不仅仅时间复杂度优化,而且只要IO一次,更好 找出TOP10:可采用内部排序,局部淘汰,堆 关于重复项的处理 分而治之+基于hash表的查找 分别遍历a,b,求hash,分到若干个小文件中 对每个小文件,将a中的数据存储到hash中,遍历b中每个数据,查询是否在hash中 若允许有一定的错误率,可采用bloom filter 是位图法的扩展 k个hash函数,每个字符串与k个bit对应,k越大,冲突的概率就越小 存在查询结果的误判,但是节省了存储开销 无需处理碰撞 $k=(ln2)*(m/n)$ 时的错误率是最小的,其中n为元素个数,m为位数组的大小,m的单位是bit,n的单位是个数,通常单个元素的长度都是狠多bit的,所以bloom filter是可以节省空间的 在错误率不大于E的情况下,$m /geq nlog_2(1/E)log_2e = nlog_2(1/E)*1.44$ 缺点:不可逆,无法恢复表示的数据,因为hash表不可逆;不能删除元素]]></content>
      <tags>
        <tag>笔记</tag>
      </tags>
  </entry>
</search>
